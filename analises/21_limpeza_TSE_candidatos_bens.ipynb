{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando os dados do TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao Índice](00_indice.ipynb)\n",
    "\n",
    "Esse notebook pega como input os dados brutos do TSE e os limpa e padroniza (coloca os dados de todos os anos, de 2004 a 2020, no mesmo formato e estrutura).\n",
    "\n",
    "Note que, nos casos de anos passados nos quais os dados eram disponibilizados sem cabeçalho, esse processo depende da existência de arquivos `header.txt` nos diretórios dos arquivos brutos. Esse arquivo não vem com os dados brutos, e foi criado com o script `criaHeader.py` (não disponível neste projeto) e os arquivos com a lista de colunas no diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "import src.xavy.utils as xu\n",
    "import src.xavy.data_retriever as xz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de carregar e preparar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tse_file(filename, header=None, conv=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a data file from TSE and a header (required if file does not \n",
    "    have a header, and this happens when the file has a .txt extension),\n",
    "    load it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if filename[-4:] == '.txt' and type(header) == type(None):\n",
    "        if verbose:\n",
    "            print('Will load header.')\n",
    "        data_dir = '/'.join(filename.split('/')[:-1])\n",
    "        header   = pd.read_csv(data_dir + '/header.txt', sep=';').columns\n",
    "    \n",
    "    # Load TSE file:\n",
    "    df = pd.read_csv(filename, sep=';', encoding='latin1', names=header, low_memory=False, converters=conv)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def substr_in_list(str_list, substr, not_in=False):\n",
    "    \"\"\"\n",
    "    Return the items in list `str_list` that contains `substr` (str).\n",
    "    \"\"\"\n",
    "    if not_in:\n",
    "        sel_str = list(filter(lambda s: s.find(substr) == -1, str_list))\n",
    "    else:\n",
    "        sel_str = list(filter(lambda s: s.find(substr) != -1, str_list))\n",
    "    return sel_str\n",
    "\n",
    "\n",
    "def n_substr_in_list(str_list, substr):\n",
    "    \"\"\"\n",
    "    Return the number of strings in list `str_list` that contains `substr` (str).\n",
    "    \"\"\"\n",
    "    sel_str = substr_in_list(str_list, substr)\n",
    "    return len(sel_str)\n",
    "\n",
    "\n",
    "def substr_in_list_Q(str_list, substr, only_one=False):\n",
    "    \"\"\"\n",
    "    Return True if `substr` (str) is a substring of any one of the items \n",
    "    in `str_list` list, or False otherwise.\n",
    "    \"\"\"\n",
    "    n = n_substr_in_list(str_list, substr)\n",
    "    if only_one & n > 1:\n",
    "        raise Exception('Found more than one item in list with \"' + substr + '\" substring.')\n",
    "    if n >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def load_tse_dir(data_dir, conv=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a directory `data_dir` with data from TSE and a file \n",
    "    called 'header.txt' containing column names if the TSE files \n",
    "    do not contain headers, load all files i the folder to a single\n",
    "    Pandas DataFrame by concatenating them.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get file list in folder:\n",
    "    file_list   = glob(data_dir + '*.txt') + glob(data_dir + '*.csv')\n",
    "    txt_A = substr_in_list_Q(file_list, '.txt')\n",
    "    csv_A = substr_in_list_Q(file_list, '.csv')\n",
    "    # Sanity checks and messages:\n",
    "    if csv_A and txt_A:\n",
    "        raise Exception(\"Found both TXT and CSV files, don't know what to do.\")\n",
    "    if verbose:\n",
    "        if txt_A:\n",
    "            print('Found TXT files. Will require header file.')\n",
    "        if csv_A:\n",
    "            print('Found CSV files.')\n",
    "            \n",
    "    # Get list of data files:\n",
    "    data_list = substr_in_list(file_list, 'header', not_in=True)\n",
    "\n",
    "    # Filter data files for BRASIL one, if existent:\n",
    "    brasil_list = substr_in_list(file_list, 'BRASIL')\n",
    "    if len(brasil_list) > 1:\n",
    "        raise Exception('Found more than one BRASIL data file.')\n",
    "    if len(brasil_list) == 1:\n",
    "        data_list = brasil_list\n",
    "        if verbose:\n",
    "            print('Found a full BRASIL file, will use it.')\n",
    "    \n",
    "    # Get header, if existent:\n",
    "    header_A = substr_in_list_Q(file_list, 'header', only_one=True)\n",
    "    if header_A:\n",
    "        if verbose:\n",
    "            print('Found header, will load it.')\n",
    "        header = pd.read_csv(data_dir + 'header.txt', sep=';').columns\n",
    "    else:\n",
    "        header = None\n",
    "\n",
    "    # Load data:\n",
    "    if verbose:\n",
    "        print('Loading data...')\n",
    "    df_list = []\n",
    "    for data_file in data_list:\n",
    "        df = load_tse_file(data_file, header, conv)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate data:\n",
    "    if verbose:\n",
    "        print('Concatenating datasets...')\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_diff(A, B):\n",
    "    \"\"\"\n",
    "    Return a list of elements in set `A` that are not in set `B`.\n",
    "    \"\"\"\n",
    "    return sorted(list(set(A) - set(B)))\n",
    "\n",
    "\n",
    "def build_col_trafo(template_header, df_header, apply_replace=True):\n",
    "    # Prefixes and terms from old files:\n",
    "    hist_col_prefix     = ['DES_SITUACAO_CANDIDATURA', 'COD_SITUACAO_CANDIDATURA',\n",
    "                           'CODIGO_', 'COD_', 'DESCRICAO_', 'DESC_', 'DES_', 'NOME_', 'NUMERO_', 'NUM_', \n",
    "                           'SIGLA_', '_SEXO', 'CPF_CANDIDATO', 'DESPESA_MAX_CAMPANHA', 'SEQUENCIAL_CANDIDATO', \n",
    "                           'COMPOSICAO_LEGENDA', 'CD_LEGENDA', 'NM_LEGENDA', 'DS_UE', 'HORA_', \n",
    "                           'DATA_GERACAO', 'DATA_NASCIMENTO', 'NR_NR_', 'VR_VR_', 'DATA_ULTIMA_ATUALIZACAO',\n",
    "                           'DETALHE_BEM', 'VALOR_BEM', 'DS_SIT_CANDIDATO', \n",
    "                           'CD_SIT_CANDIDATO', '_SIT_CAND_SUPERIOR', '_SIT_CAND_TOT', 'SEQUENCIAL_LEGENDA',\n",
    "                           'TRANSITO', 'TOTAL_VOTOS', 'NR_CAND', 'NR_CANDIDATOIDATO',\n",
    "                           'ST_VOTO_EM_ST_VOTO_EM_TRANSITO']\n",
    "    \n",
    "    # Respective prefixes and terms for template:\n",
    "    cand2020_col_prefix = ['DS_DETALHE_SITUACAO_CAND', 'CD_DETALHE_SITUACAO_CAND',\n",
    "                           'CD_', 'CD_', 'DS_', 'DS_', 'DS_', 'NM_', 'NR_', 'NR_', \n",
    "                           'SG_', '_GENERO', 'NR_CPF_CANDIDATO', 'VR_DESPESA_MAX_CAMPANHA', 'SQ_CANDIDATO', \n",
    "                           'DS_COMPOSICAO_COLIGACAO', 'SQ_COLIGACAO', 'NM_COLIGACAO', 'NM_UE', 'HH_', \n",
    "                           'DT_GERACAO', 'DT_NASCIMENTO', 'NR_', 'VR_', 'DT_ULTIMA_ATUALIZACAO',\n",
    "                           'DS_BEM_CANDIDATO', 'VR_BEM_CANDIDATO', 'DS_DETALHE_SITUACAO_CAND',\n",
    "                           'CD_DETALHE_SITUACAO_CAND', '_SITUACAO_CANDIDATURA', '_SIT_TOT_TURNO', 'SQ_COLIGACAO',\n",
    "                           'ST_VOTO_EM_TRANSITO', 'QT_VOTOS_NOMINAIS', 'NR_CANDIDATO', 'NR_CANDIDATO',\n",
    "                           'ST_VOTO_EM_TRANSITO']\n",
    "    \n",
    "    if apply_replace:\n",
    "        # Replace all olf prefixes and terms for new ones:\n",
    "        new_hist_cols   = xu.mass_replace(df_header, hist_col_prefix, cand2020_col_prefix)\n",
    "    else:\n",
    "        new_hist_cols   = df_header\n",
    "        \n",
    "    # Get columns lost in template:\n",
    "    extra_cols = set_diff(new_hist_cols, template_header)\n",
    "\n",
    "    return new_hist_cols, extra_cols\n",
    "\n",
    "\n",
    "def standardize_df_columns(df, template_header, old_cols):\n",
    "    \"\"\"\n",
    "    Transform the `df` (DataFrame) header so it matches the `template_header`\n",
    "    (list of str) appended with extra columns in `old_cols` (list of str). \n",
    "    \"\"\"\n",
    "    # Get df header:\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Get new names for df columns and columns not present in template header:\n",
    "    new_df_cols, extra_cols = build_col_trafo(template_header, df_header)\n",
    "    # Build a dict to translate df column names to template style:\n",
    "    col_mapper = dict(zip(df_header, new_df_cols))\n",
    "    \n",
    "    if len(set(extra_cols) - set(old_cols)) > 0:\n",
    "        raise Exception('There are columns in DataFrame that are not accounted for.')\n",
    "    \n",
    "    # Create standard table header:\n",
    "    template_frame = pd.DataFrame(data=[], columns=list(template_header) + old_cols)\n",
    "\n",
    "    # Translate columns in df to template style:\n",
    "    df = df.rename(col_mapper, axis=1)\n",
    "    # Put df under standard header:\n",
    "    df = pd.concat([template_frame, df])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def format_date(date):\n",
    "    \"\"\"\n",
    "    Standardize format for (birth) dates that are in weird or inconsistent formats,\n",
    "    specially in old databases.\n",
    "    \"\"\"\n",
    "    # Para mudar formatação de data fora do padrão:\n",
    "    MonthDic = {'JAN':'01', 'FEB':'02', 'MAR':'03', 'APR':'04', 'MAY':'05', 'JUN':'06', \n",
    "                'JUL':'07', 'AUG':'08', 'SEP':'09', 'OCT':'10', 'NOV':'11', 'DEC':'12'}\n",
    "    MonthMax = {'01':31,'02':29,'03':31,'04':30,'05':31,'06':30,\n",
    "                '07':31,'08':31,'09':30,'10':31,'11':30,'12':31}\n",
    "\n",
    "    try:\n",
    "        # Caso campo em branco:\n",
    "        if type(date)==float and np.isnan(date):\n",
    "            return ''\n",
    "        # Caso campo contendo espaços:\n",
    "        if date.find(' ')!=-1:\n",
    "            if len(date)==7:\n",
    "                date = date.replace(' ', '')\n",
    "                togo = ['0'+date[0:1],'0'+date[1:2],date[2:]]\n",
    "            else:\n",
    "                togo = [str(int(date[:2])).zfill(2),str(int(date[2:4])).zfill(2),date[4:]]\n",
    "        # Caso campo apenas números:\n",
    "        elif date.isdigit()==True:\n",
    "            if len(date)==6:\n",
    "                if date[2:4]=='19':\n",
    "                    togo = ['0'+date[0:1],'0'+date[1:2],date[2:]]\n",
    "                else:\n",
    "                    togo = [date[:2],date[2:4],'19'+date[4:]]\n",
    "            elif len(date)==7:\n",
    "                if date[3:5]=='19':\n",
    "                    if int(date[1:3])>12:\n",
    "                        togo = [date[:2],'0'+date[2:3],date[3:]]\n",
    "                    else:\n",
    "                        togo = ['0'+date[:1],date[1:3],date[3:]]\n",
    "                elif date[4:6]=='19':\n",
    "                    togo = [date[:2],date[2:4],date[4:]+'0']\n",
    "                else:\n",
    "                    print('Data ininteligível 1:', date)\n",
    "                    return date\n",
    "            else:\n",
    "                togo = [date[:2],date[2:4],date[4:]]\n",
    "            if int(togo[0])>MonthMax[togo[1]]:\n",
    "                togo[0]=str(int(togo[0])-1)\n",
    "            if togo[0]=='29' and togo[1]=='02' and (int(togo[2])-1900)%4!=0:\n",
    "                togo[0]='28'\n",
    "        # Caso separação por traços:\n",
    "        elif date.find('-')!=-1:\n",
    "            togo = date.split('-')\n",
    "            if togo[1].isdigit()==False:\n",
    "                togo[1] = MonthDic[togo[1]]\n",
    "                if len(togo[2])==2:\n",
    "                    togo[2] = '19'+togo[2]\n",
    "                else:\n",
    "                    print('Data ininteligível 2:', date)\n",
    "            else:\n",
    "                print('Data ininteligível 3:', date)\n",
    "        # Caso separação por barra:\n",
    "        else:\n",
    "            togo = date.split('/')\n",
    "        # Casos de ano apenas com dois dígitos:\n",
    "        if int(togo[2])<1800 or int(togo[2])>2100:\n",
    "            return togo[0]+'/'+togo[1]+'/19'+togo[2][2:]\n",
    "        # Caso ano com 4 dígitos:\n",
    "        else:\n",
    "            return '/'.join(togo)\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def clean_tit_eleitor(tit_eleitor_str):\n",
    "    \"\"\"\n",
    "    Strip whitespaces from Título de eleitor `tit_eleitor_str` (str)\n",
    "    and pad with zeros (to get 12 digits).\n",
    "    \"\"\"\n",
    "    tit_eleitor_str = ''.join(tit_eleitor_str.split())\n",
    "    tit_eleitor_str = tit_eleitor_str.rjust(12,'0')\n",
    "    return tit_eleitor_str\n",
    "\n",
    "def clean_cpf(cpf_series):\n",
    "    \"\"\"\n",
    "    Apply hard-coded translation of CPFs to a Pandas Series `cpf_series`.\n",
    "    The hard-coded dict was created based on the search for other entries \n",
    "    (in other years) with same name and Tit. eleitor.\n",
    "    \"\"\"\n",
    "    cpf_fixer = {'48 110000':'53909542549', '60 30434549':'60630434549', '58 753000':'68411537404'}\n",
    "    cpf_series = cpf_series.map(xu.translate_dict(cpf_fixer))\n",
    "    return cpf_series\n",
    "\n",
    "\n",
    "def clean_votos_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"votacao_candidato_munzona\" dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SG_UE for 2006 is wrong, fix it:\n",
    "    df.loc[df['ANO_ELEICAO'] == 2006, 'SG_UE'] = df.loc[df['ANO_ELEICAO'] == 2006, 'SG_UF']\n",
    "    \n",
    "    # Standardize nome do cargo:\n",
    "    df['DS_CARGO'] = df['DS_CARGO'].str.upper()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_cand_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"consulta_cand\" DataFrame.\n",
    "    \"\"\"\n",
    "    # Fix birth dates manually:\n",
    "    bad_date = ['3010196', '3010179', '2801 966', '29021954', '29021947']\n",
    "    fix_date = ['30101960', '30101979', '28011966', '28021954', '28021947']\n",
    "    for bad, fix in zip(bad_date, fix_date):\n",
    "        df.loc[df['DT_NASCIMENTO'] == bad, 'DT_NASCIMENTO'] = fix\n",
    "    # Fix birth dates with formula:\n",
    "    df['DT_NASCIMENTO'] = df['DT_NASCIMENTO'].apply(format_date)\n",
    "\n",
    "    # Fix level of education:\n",
    "    df['DS_GRAU_INSTRUCAO'] = df['DS_GRAU_INSTRUCAO'].str.replace('^FUNDAMENTAL', 'ENSINO FUNDAMENTAL', regex=True)\n",
    "    df['DS_GRAU_INSTRUCAO'] = df['DS_GRAU_INSTRUCAO'].str.replace('^MÉDIO', 'ENSINO MÉDIO', regex=True)\n",
    "\n",
    "    # Clean documents (CPF and Tit Eleitor):\n",
    "    df['NR_TITULO_ELEITORAL_CANDIDATO'] = df['NR_TITULO_ELEITORAL_CANDIDATO'].apply(clean_tit_eleitor)\n",
    "    df['NR_CPF_CANDIDATO'] = clean_cpf(df['NR_CPF_CANDIDATO'])\n",
    "    \n",
    "    # Standardize descrição do cargo:\n",
    "    df['DS_CARGO'] = df['DS_CARGO'].str.replace('º SUPLENTE SENADOR', 'º SUPLENTE')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_bem_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"bem_candidato\" DataFrame.\n",
    "    \"\"\"\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].str.replace(',', '.')\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].astype(float)\n",
    "    # Some values are negative, but by the looks of it, this is a mistake:\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].abs()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_header_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load the header associated with a single TSE data file `filename` (str).\n",
    "    \"\"\"\n",
    "    return load_tse_file(filename).columns\n",
    "\n",
    "\n",
    "def load_header_from_dir(folder):\n",
    "    \"\"\"\n",
    "    Load the header associated with a folder of TSE data files `folder` (str).\n",
    "    \"\"\"\n",
    "    return load_tse_dir(folder).columns\n",
    "\n",
    "\n",
    "def clear_all_null_tokens(df):\n",
    "    \"\"\"\n",
    "    Replace all known TSE tokens that represent missing values with None.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        df.loc[df[col].isin([-1, -3, '-1', '-3', '#NULO#', '#NE#', '#NI#', '00000000#NI#', '0000000000-1']), col] = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def etl_cand(folder, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidates (consulta_cand).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    folder : str\n",
    "        Path to the folder containing the data. For data files that do not contain headers\n",
    "        (.txt files), the folder should also contain a 'header.txt' file with the header \n",
    "        in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a single file that will receive all the data in `folder`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    consulta_cand_old_cols = ['IDADE_DATA_ELEICAO', 'SG_LEGENDA']\n",
    "    # How to read in certain columns:\n",
    "    cand_types = {'DATA_NASCIMENTO':str, 'CPF_CANDIDATO':str, 'NUM_TITULO_ELEITORAL_CANDIDATO':str,\n",
    "                  'NR_TITULO_ELEITORAL_CANDIDATO': str, 'NR_CPF_CANDIDATO': str}    \n",
    "    \n",
    "    # Load data from folder:\n",
    "    df = load_tse_dir(folder, cand_types, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    trans = standardize_df_columns(df, template_header, consulta_cand_old_cols)\n",
    "    trans = clean_cand_df(trans)\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "\n",
    "def etl_bem(folder, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidates' wealth (bem_candidato).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    folder : str\n",
    "        Path to the folder containing the data. For data files that do not contain headers\n",
    "        (.txt files), the folder should also contain a 'header.txt' file with the header \n",
    "        in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a single file that will receive all the data in `folder`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    bem_candidato_old_cols = []\n",
    "    # How to read in certain columns:\n",
    "    bem_types  = {'VALOR_BEM':str, 'VR_BEM_CANDIDATO':str}\n",
    "    \n",
    "    # Load data from folder:\n",
    "    print('Load data...')\n",
    "    df = load_tse_dir(folder, bem_types, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    print('Standardize...')\n",
    "    trans = standardize_df_columns(df, template_header, bem_candidato_old_cols)\n",
    "    print('Clean...')\n",
    "    trans = clean_bem_df(trans)\n",
    "    print('Clear nulls...')\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    print('Save file...')\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "\n",
    "def etl_votos(infile, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidate's votes (votacao_candidato_munzona).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    infile : str\n",
    "        Path to the data file. For data files that do not contain headers\n",
    "        (.txt files), the folder containing the file should also contain a 'header.txt' \n",
    "        file with the header in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a file that will receive all the  cleaned data from `infile`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    votacao_old_cols = []\n",
    "    \n",
    "    # Load data from folder:\n",
    "    df = load_tse_file(infile, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    trans = standardize_df_columns(df, template_header, votacao_old_cols)\n",
    "    trans = clean_votos_df(trans)\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpa dados do TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os dados brutos originais foram obtidos do TSE: <https://dadosabertos.tse.jus.br/dataset/>. Abaixo, os dados são baixados da nuvem do Ceweb."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a local file, skip download.\n"
     ]
    }
   ],
   "source": [
    "# Dados das candidaturas utilizados na análise, produzidos pelo TSE logo após o término do período de registro:\n",
    "xz.sync_remote_zipped_files('https://nuvem.ceweb.br/s/NspgTDAow7d7mNQ/download/consulta_cand_2022.zip', '../dados/brutos/tse/', keep_zip_dir=False)\n",
    "cand_header = load_header_from_dir('../dados/brutos/tse/consulta_cand_2022/')\n",
    "# Para salvar o resultado, descomente a linha abaixo (ela fica comentada para evitar sobrescrever o arquivo por acidente):\n",
    "#etl_cand('../dados/brutos/tse/consulta_cand_2022/', cand_header, '../dados/limpos/tse/consulta_cand/consulta_cand_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados das candidaturas utilizadas de maneira auxiliar, produzidos pelo TSE após o resultado das eleições:\n",
    "#cand_header = load_header_from_dir('/home/skems/ceweb/dados/brutos/tse/consulta_cand_2022/')\n",
    "#etl_cand('/home/hxavier/ceweb/dados/brutos/tse/bkp/consulta_cand_2022_v04/', cand_header, '/home/hxavier/ceweb/dados/limpos/tse/consulta_cand/consulta_cand_2022_BRASIL_pos.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a local file, skip download.\n"
     ]
    }
   ],
   "source": [
    "xz.sync_remote_zipped_files('https://nuvem.ceweb.br/s/NwbEzCQCBqYJ3s6/download/bem_candidato_2022.zip', '../dados/brutos/tse/', keep_zip_dir=False)\n",
    "bem_header = load_header_from_dir('../dados/brutos/tse/bem_candidato_2022/')\n",
    "#etl_bem('../dados/brutos/tse/bem_candidato_2022/', bem_header, '../dados/limpos/tse/bens_candidatos/bem_candidato_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.xavy.explore as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found a local file, skip download.\n"
     ]
    }
   ],
   "source": [
    "xz.sync_remote_zipped_files('https://nuvem.ceweb.br/s/xq6nGLHsEZpgMHo/download/bem_candidato_2022_BRASIL.zip', '../dados/limpos/tse/bens_candidatos/', keep_zip_dir=False)\n",
    "bens_df = pd.read_csv('../dados/limpos/tse/bens_candidatos/bem_candidato_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "              coluna    N     %\n",
      "15  DS_BEM_CANDIDATO  6.0  0.01\n"
     ]
    }
   ],
   "source": [
    "xe.checkMissing(bens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m28\u001b[0m: AC\u001b[1;34m | \u001b[0mAL\u001b[1;34m | \u001b[0mAM\u001b[1;34m | \u001b[0mAP\u001b[1;34m | \u001b[0mBA\u001b[1;34m | \u001b[0mBR\u001b[1;34m | \u001b[0mCE\u001b[1;34m | \u001b[0mDF\u001b[1;34m | \u001b[0mES\u001b[1;34m | \u001b[0mGO\u001b[1;34m | \u001b[0mMA\u001b[1;34m | \u001b[0mMG\u001b[1;34m | \u001b[0mMS\u001b[1;34m | \u001b[0mMT\u001b[1;34m | \u001b[0mPA\u001b[1;34m | \u001b[0mPB\u001b[1;34m | \u001b[0mPE\u001b[1;34m | \u001b[0mPI\u001b[1;34m | \u001b[0mPR\u001b[1;34m | \u001b[0mRJ\u001b[1;34m | \u001b[0mRN\u001b[1;34m | \u001b[0mRO\u001b[1;34m | \u001b[0mRR\u001b[1;34m | \u001b[0mRS\u001b[1;34m | \u001b[0mSC\u001b[1;34m | \u001b[0mSE\u001b[1;34m | \u001b[0mSP\u001b[1;34m | \u001b[0mTO\n"
     ]
    }
   ],
   "source": [
    "xe.unique(bens_df['SG_UF'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDT_GERACAO: \u001b[0m1 unique values.\n",
      "06/10/2022\n",
      "\n",
      "\u001b[1mHH_GERACAO: \u001b[0m1 unique values.\n",
      "13:09:54\n",
      "\n",
      "\u001b[1mANO_ELEICAO: \u001b[0m1 unique values.\n",
      "2022\n",
      "\n",
      "\u001b[1mCD_TIPO_ELEICAO: \u001b[0m1 unique values.\n",
      "2\n",
      "\n",
      "\u001b[1mNM_TIPO_ELEICAO: \u001b[0m1 unique values.\n",
      "Eleição Ordinária\n",
      "\n",
      "\u001b[1mCD_ELEICAO: \u001b[0m2 unique values.\n",
      "544,  546\n",
      "\n",
      "\u001b[1mDS_ELEICAO: \u001b[0m2 unique values.\n",
      "Eleição Geral Federal 2022,  Eleições Gerais Estaduais 2022\n",
      "\n",
      "\u001b[1mDT_ELEICAO: \u001b[0m1 unique values.\n",
      "02/10/2022\n",
      "\n",
      "\u001b[1mSG_UF: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mAL,  AM,  AP,  BR,  CE,  DF,  ES,  GO,  MA,  MS,  MT,  PA,  PE,  PI,  PR,  RJ,  RN,  RO,  RS,  SE\n",
      "\n",
      "\u001b[1mSG_UE: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mAC,  AL,  AM,  AP,  BA,  BR,  CE,  MA,  MG,  MS,  MT,  PB,  PE,  PI,  PR,  RN,  RO,  SC,  SE,  TO\n",
      "\n",
      "\u001b[1mNM_UE: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mALAGOAS,  AMAZONAS,  BAHIA,  BRASIL,  CEARÁ,  DISTRITO FEDERAL,  ESPÍRITO SANTO,  MINAS GERAIS,  PARANÁ,  PARAÍBA,  PARÁ,  PIAUÍ,  RIO DE JANEIRO,  RIO GRANDE DO NORTE,  RIO GRANDE DO SUL,  RONDÔNIA,  RORAIMA,  SANTA CATARINA,  SERGIPE,  TOCANTINS\n",
      "\n",
      "\u001b[1mSQ_CANDIDATO: \u001b[0m18249 unique values.\n",
      "\u001b[1m(sample) \u001b[0m50001602967,  50001610387,  50001614019,  50001619276,  60001603451,  70001650221,  80001722143,  110001601580,  110001701631,  130001607530,  140001615532,  160001621929,  170001715431,  190001644824,  200001618064,  210001649221,  250001610131,  250001611961,  250001620261,  250001642214\n",
      "\n",
      "\u001b[1mNR_ORDEM_CANDIDATO: \u001b[0m180 unique values.\n",
      "\u001b[1m(sample) \u001b[0m6,  16,  18,  27,  33,  35,  37,  46,  48,  68,  78,  79,  95,  107,  108,  134,  140,  162,  173,  177\n",
      "\n",
      "\u001b[1mCD_TIPO_BEM_CANDIDATO: \u001b[0m50 unique values.\n",
      "\u001b[1m(sample) \u001b[0m3,  13,  17,  21,  26,  29,  32,  41,  45,  47,  49,  51,  54,  62,  69,  91,  93,  94,  97,  99\n",
      "\n",
      "\u001b[1mDS_TIPO_BEM_CANDIDATO: \u001b[0m50 unique values.\n",
      "\u001b[1m(sample) \u001b[0mBem relacionado com o exercício da atividade autônoma,  Casa,  Depósito bancário em conta corrente no exterior,  Dinheiro em espécie - moeda estrangeira,  Dinheiro em espécie - moeda nacional,  Direito de autor, de inventor e patente,  Direito de lavra e assemelhado,  Fundos: Ações, Mútuos de Privatização, Invest. Empresas Emergentes, Invest.Participação e Invest. Índice Mercado,  Jóia, quadro, objeto de arte, de coleção, antiguidade, etc.,  Linha telefônica,  Loja,  Outras participações societárias,  Outros créditos e poupança vinculados,  Outros depósitos à vista e numerário,  Outros fundos,  Prédio comercial,  Sala ou conjunto,  Terra nua,  Terreno,  Veículo automotor terrestre: caminhão, automóvel, moto, etc.\n",
      "\n",
      "\u001b[1mDS_BEM_CANDIDATO: \u001b[0m69977 unique values.\n",
      "\u001b[1m(sample) \u001b[0m701 Porto Alegre,  75% - arbore,  ACOES CSMG3,  AÇÕES BOVESPA PETR4, VALE3,  CAIXA ECONOMICA FEDERAL AGENCIA 1457-5 CONTA 013/00032147-8,  CASA PENDENTE DE ESCRITURA COM VÁRIAS BENFEITORIAS,  CC2,  COTA UNIMED,  EMPRESTIMO EM 72 PARCELAS NO VALOR DE 3.571,00,  FORD F1000,  GREENVILE PATAMARES,  ITAU UNIBANCO S/A - C/C,  LOCALIZADO NA CIDADE DE SOROCABA,  LOTE 3 - QUADRA 3 - LOTEAMENTO FRADINHOS - VITORIA,  PARTICIPAÇÃO EM COTAS DE EMPRESA,  PGBL - Bradesco,  SALA 404 NO EDF. SALVADOR TRADE CENTER, AV TANCREDONEVES NO 1632 TORRE NORTE, ADQ. DA ODEBRECHT EM 2003.,  SALDO EM CONTA CORRENTE - ORIUNDO DE PARTICIPAÇÃO E LUCRO EM EMPRESA,  Um imovel residencial R. Major Joaquim B. de Carvalho 194 Vila Angelica em SJ Rio Preto SP adquirido em 05/1983,  participação societária, Aplha Comercio de Combustiveis LTDA CNPJ 09.148.368/0001-76\n",
      "\n",
      "\u001b[1mVR_BEM_CANDIDATO: \u001b[0m37821 unique values.\n",
      "\u001b[1m(sample) \u001b[0m53.09,  393.15,  750.0,  1117.2,  1627.81,  1664.0,  2759.17,  10141.0,  16002.86,  48697.43,  60500.0,  73250.0,  182795.85,  229120.47,  360923.24,  362086.34,  378526.1,  1158182.0,  6300000.0,  13712659.0\n",
      "\n",
      "\u001b[1mDT_ULTIMA_ATUALIZACAO: \u001b[0m18 unique values.\n",
      "02/10/2022,  03/10/2022,  04/10/2022,  05/10/2022,  06/10/2022,  08/09/2022,  10/09/2022,  13/09/2022,  14/09/2022,  15/09/2022,  16/09/2022,  17/09/2022,  18/09/2022,  20/09/2022,  22/09/2022,  23/09/2022,  28/09/2022,  30/09/2022\n",
      "\n",
      "\u001b[1mHH_ULTIMA_ATUALIZACAO: \u001b[0m153 unique values.\n",
      "\u001b[1m(sample) \u001b[0m01:10:56,  01:38:42,  09:27:04,  12:05:36,  12:40:51,  15:48:44,  15:48:47,  15:48:52,  15:49:00,  15:49:08,  15:52:38,  16:40:16,  17:39:21,  18:27:12,  18:41:02,  19:24:34,  20:25:38,  20:46:21,  20:57:02,  23:16:22\n"
     ]
    }
   ],
   "source": [
    "xe.mapUnique(bens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
