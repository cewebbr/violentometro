{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Limpando os dados do TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Voltar ao Índice](00_indice.ipynb)\n",
    "\n",
    "Esse notebook pega como input os dados brutos do TSE e os limpa e padroniza (coloca os dados de todos os anos, de 2004 a 2020, no mesmo formato e estrutura).\n",
    "\n",
    "Note que, nos casos de anos passados nos quais os dados eram disponibilizados sem cabeçalho, esse processo depende da existência de arquivos `header.txt` nos diretórios dos arquivos brutos. Esse arquivo não vem com os dados brutos, e foi criado com o script `../scripts/criaHeader.py` e os arquivos com a lista de colunas no diretório `../dados/aux/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from glob import glob\n",
    "\n",
    "import utils as xu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de plotagem\n",
    "\n",
    "Para testar e comparar a composição das colunas. Foram movidas para o arquivo `plot_comparison.py`. Isso pode causar problemas abaixo, mas basta carregar o módulo e usá-lo para resolver o problema."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de carregar e preparar dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tse_file(filename, header=None, conv=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a data file from TSE and a header (required if file does not \n",
    "    have a header, and this happens when the file has a .txt extension),\n",
    "    load it.\n",
    "    \"\"\"\n",
    "    \n",
    "    if filename[-4:] == '.txt' and type(header) == type(None):\n",
    "        if verbose:\n",
    "            print('Will load header.')\n",
    "        data_dir = '/'.join(filename.split('/')[:-1])\n",
    "        header   = pd.read_csv(data_dir + '/header.txt', sep=';').columns\n",
    "    \n",
    "    # Load TSE file:\n",
    "    df = pd.read_csv(filename, sep=';', encoding='latin1', names=header, low_memory=False, converters=conv)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def substr_in_list(str_list, substr, not_in=False):\n",
    "    \"\"\"\n",
    "    Return the items in list `str_list` that contains `substr` (str).\n",
    "    \"\"\"\n",
    "    if not_in:\n",
    "        sel_str = list(filter(lambda s: s.find(substr) == -1, str_list))\n",
    "    else:\n",
    "        sel_str = list(filter(lambda s: s.find(substr) != -1, str_list))\n",
    "    return sel_str\n",
    "\n",
    "\n",
    "def n_substr_in_list(str_list, substr):\n",
    "    \"\"\"\n",
    "    Return the number of strings in list `str_list` that contains `substr` (str).\n",
    "    \"\"\"\n",
    "    sel_str = substr_in_list(str_list, substr)\n",
    "    return len(sel_str)\n",
    "\n",
    "\n",
    "def substr_in_list_Q(str_list, substr, only_one=False):\n",
    "    \"\"\"\n",
    "    Return True if `substr` (str) is a substring of any one of the items \n",
    "    in `str_list` list, or False otherwise.\n",
    "    \"\"\"\n",
    "    n = n_substr_in_list(str_list, substr)\n",
    "    if only_one & n > 1:\n",
    "        raise Exception('Found more than one item in list with \"' + substr + '\" substring.')\n",
    "    if n >= 1:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    \n",
    "def load_tse_dir(data_dir, conv=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Given a directory `data_dir` with data from TSE and a file \n",
    "    called 'header.txt' containing column names if the TSE files \n",
    "    do not contain headers, load all files i the folder to a single\n",
    "    Pandas DataFrame by concatenating them.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get file list in folder:\n",
    "    file_list   = glob(data_dir + '*.txt') + glob(data_dir + '*.csv')\n",
    "    txt_A = substr_in_list_Q(file_list, '.txt')\n",
    "    csv_A = substr_in_list_Q(file_list, '.csv')\n",
    "    # Sanity checks and messages:\n",
    "    if csv_A and txt_A:\n",
    "        raise Exception(\"Found both TXT and CSV files, don't know what to do.\")\n",
    "    if verbose:\n",
    "        if txt_A:\n",
    "            print('Found TXT files. Will require header file.')\n",
    "        if csv_A:\n",
    "            print('Found CSV files.')\n",
    "            \n",
    "    # Get list of data files:\n",
    "    data_list = substr_in_list(file_list, 'header', not_in=True)\n",
    "\n",
    "    # Filter data files for BRASIL one, if existent:\n",
    "    brasil_list = substr_in_list(file_list, 'BRASIL')\n",
    "    if len(brasil_list) > 1:\n",
    "        raise Exception('Found more than one BRASIL data file.')\n",
    "    if len(brasil_list) == 1:\n",
    "        data_list = brasil_list\n",
    "        if verbose:\n",
    "            print('Found a full BRASIL file, will use it.')\n",
    "    \n",
    "    # Get header, if existent:\n",
    "    header_A = substr_in_list_Q(file_list, 'header', only_one=True)\n",
    "    if header_A:\n",
    "        if verbose:\n",
    "            print('Found header, will load it.')\n",
    "        header = pd.read_csv(data_dir + 'header.txt', sep=';').columns\n",
    "    else:\n",
    "        header = None\n",
    "\n",
    "    # Load data:\n",
    "    if verbose:\n",
    "        print('Loading data...')\n",
    "    df_list = []\n",
    "    for data_file in data_list:\n",
    "        df = load_tse_file(data_file, header, conv)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # Concatenate data:\n",
    "    if verbose:\n",
    "        print('Concatenating datasets...')\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def set_diff(A, B):\n",
    "    \"\"\"\n",
    "    Return a list of elements in set `A` that are not in set `B`.\n",
    "    \"\"\"\n",
    "    return sorted(list(set(A) - set(B)))\n",
    "\n",
    "\n",
    "def build_col_trafo(template_header, df_header, apply_replace=True):\n",
    "    # Prefixes and terms from old files:\n",
    "    hist_col_prefix     = ['DES_SITUACAO_CANDIDATURA', 'COD_SITUACAO_CANDIDATURA',\n",
    "                           'CODIGO_', 'COD_', 'DESCRICAO_', 'DESC_', 'DES_', 'NOME_', 'NUMERO_', 'NUM_', \n",
    "                           'SIGLA_', '_SEXO', 'CPF_CANDIDATO', 'DESPESA_MAX_CAMPANHA', 'SEQUENCIAL_CANDIDATO', \n",
    "                           'COMPOSICAO_LEGENDA', 'CD_LEGENDA', 'NM_LEGENDA', 'DS_UE', 'HORA_', \n",
    "                           'DATA_GERACAO', 'DATA_NASCIMENTO', 'NR_NR_', 'VR_VR_', 'DATA_ULTIMA_ATUALIZACAO',\n",
    "                           'DETALHE_BEM', 'VALOR_BEM', 'DS_SIT_CANDIDATO', \n",
    "                           'CD_SIT_CANDIDATO', '_SIT_CAND_SUPERIOR', '_SIT_CAND_TOT', 'SEQUENCIAL_LEGENDA',\n",
    "                           'TRANSITO', 'TOTAL_VOTOS', 'NR_CAND', 'NR_CANDIDATOIDATO',\n",
    "                           'ST_VOTO_EM_ST_VOTO_EM_TRANSITO']\n",
    "    \n",
    "    # Respective prefixes and terms for template:\n",
    "    cand2020_col_prefix = ['DS_DETALHE_SITUACAO_CAND', 'CD_DETALHE_SITUACAO_CAND',\n",
    "                           'CD_', 'CD_', 'DS_', 'DS_', 'DS_', 'NM_', 'NR_', 'NR_', \n",
    "                           'SG_', '_GENERO', 'NR_CPF_CANDIDATO', 'VR_DESPESA_MAX_CAMPANHA', 'SQ_CANDIDATO', \n",
    "                           'DS_COMPOSICAO_COLIGACAO', 'SQ_COLIGACAO', 'NM_COLIGACAO', 'NM_UE', 'HH_', \n",
    "                           'DT_GERACAO', 'DT_NASCIMENTO', 'NR_', 'VR_', 'DT_ULTIMA_ATUALIZACAO',\n",
    "                           'DS_BEM_CANDIDATO', 'VR_BEM_CANDIDATO', 'DS_DETALHE_SITUACAO_CAND',\n",
    "                           'CD_DETALHE_SITUACAO_CAND', '_SITUACAO_CANDIDATURA', '_SIT_TOT_TURNO', 'SQ_COLIGACAO',\n",
    "                           'ST_VOTO_EM_TRANSITO', 'QT_VOTOS_NOMINAIS', 'NR_CANDIDATO', 'NR_CANDIDATO',\n",
    "                           'ST_VOTO_EM_TRANSITO']\n",
    "    \n",
    "    if apply_replace:\n",
    "        # Replace all olf prefixes and terms for new ones:\n",
    "        new_hist_cols   = xu.mass_replace(df_header, hist_col_prefix, cand2020_col_prefix)\n",
    "    else:\n",
    "        new_hist_cols   = df_header\n",
    "        \n",
    "    # Get columns lost in template:\n",
    "    extra_cols = set_diff(new_hist_cols, template_header)\n",
    "\n",
    "    return new_hist_cols, extra_cols\n",
    "\n",
    "\n",
    "def standardize_df_columns(df, template_header, old_cols):\n",
    "    \"\"\"\n",
    "    Transform the `df` (DataFrame) header so it matches the `template_header`\n",
    "    (list of str) appended with extra columns in `old_cols` (list of str). \n",
    "    \"\"\"\n",
    "    # Get df header:\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Get new names for df columns and columns not present in template header:\n",
    "    new_df_cols, extra_cols = build_col_trafo(template_header, df_header)\n",
    "    # Build a dict to translate df column names to template style:\n",
    "    col_mapper = dict(zip(df_header, new_df_cols))\n",
    "    \n",
    "    if len(set(extra_cols) - set(old_cols)) > 0:\n",
    "        raise Exception('There are columns in DataFrame that are not accounted for.')\n",
    "    \n",
    "    # Create standard table header:\n",
    "    template_frame = pd.DataFrame(data=[], columns=list(template_header) + old_cols)\n",
    "\n",
    "    # Translate columns in df to template style:\n",
    "    df = df.rename(col_mapper, axis=1)\n",
    "    # Put df under standard header:\n",
    "    df = pd.concat([template_frame, df])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def format_date(date):\n",
    "    \"\"\"\n",
    "    Standardize format for (birth) dates that are in weird or inconsistent formats,\n",
    "    specially in old databases.\n",
    "    \"\"\"\n",
    "    # Para mudar formatação de data fora do padrão:\n",
    "    MonthDic = {'JAN':'01', 'FEB':'02', 'MAR':'03', 'APR':'04', 'MAY':'05', 'JUN':'06', \n",
    "                'JUL':'07', 'AUG':'08', 'SEP':'09', 'OCT':'10', 'NOV':'11', 'DEC':'12'}\n",
    "    MonthMax = {'01':31,'02':29,'03':31,'04':30,'05':31,'06':30,\n",
    "                '07':31,'08':31,'09':30,'10':31,'11':30,'12':31}\n",
    "\n",
    "    try:\n",
    "        # Caso campo em branco:\n",
    "        if type(date)==float and np.isnan(date):\n",
    "            return ''\n",
    "        # Caso campo contendo espaços:\n",
    "        if date.find(' ')!=-1:\n",
    "            if len(date)==7:\n",
    "                date = date.replace(' ', '')\n",
    "                togo = ['0'+date[0:1],'0'+date[1:2],date[2:]]\n",
    "            else:\n",
    "                togo = [str(int(date[:2])).zfill(2),str(int(date[2:4])).zfill(2),date[4:]]\n",
    "        # Caso campo apenas números:\n",
    "        elif date.isdigit()==True:\n",
    "            if len(date)==6:\n",
    "                if date[2:4]=='19':\n",
    "                    togo = ['0'+date[0:1],'0'+date[1:2],date[2:]]\n",
    "                else:\n",
    "                    togo = [date[:2],date[2:4],'19'+date[4:]]\n",
    "            elif len(date)==7:\n",
    "                if date[3:5]=='19':\n",
    "                    if int(date[1:3])>12:\n",
    "                        togo = [date[:2],'0'+date[2:3],date[3:]]\n",
    "                    else:\n",
    "                        togo = ['0'+date[:1],date[1:3],date[3:]]\n",
    "                elif date[4:6]=='19':\n",
    "                    togo = [date[:2],date[2:4],date[4:]+'0']\n",
    "                else:\n",
    "                    print('Data ininteligível 1:', date)\n",
    "                    return date\n",
    "            else:\n",
    "                togo = [date[:2],date[2:4],date[4:]]\n",
    "            if int(togo[0])>MonthMax[togo[1]]:\n",
    "                togo[0]=str(int(togo[0])-1)\n",
    "            if togo[0]=='29' and togo[1]=='02' and (int(togo[2])-1900)%4!=0:\n",
    "                togo[0]='28'\n",
    "        # Caso separação por traços:\n",
    "        elif date.find('-')!=-1:\n",
    "            togo = date.split('-')\n",
    "            if togo[1].isdigit()==False:\n",
    "                togo[1] = MonthDic[togo[1]]\n",
    "                if len(togo[2])==2:\n",
    "                    togo[2] = '19'+togo[2]\n",
    "                else:\n",
    "                    print('Data ininteligível 2:', date)\n",
    "            else:\n",
    "                print('Data ininteligível 3:', date)\n",
    "        # Caso separação por barra:\n",
    "        else:\n",
    "            togo = date.split('/')\n",
    "        # Casos de ano apenas com dois dígitos:\n",
    "        if int(togo[2])<1800 or int(togo[2])>2100:\n",
    "            return togo[0]+'/'+togo[1]+'/19'+togo[2][2:]\n",
    "        # Caso ano com 4 dígitos:\n",
    "        else:\n",
    "            return '/'.join(togo)\n",
    "    \n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    \n",
    "def clean_tit_eleitor(tit_eleitor_str):\n",
    "    \"\"\"\n",
    "    Strip whitespaces from Título de eleitor `tit_eleitor_str` (str)\n",
    "    and pad with zeros (to get 12 digits).\n",
    "    \"\"\"\n",
    "    tit_eleitor_str = ''.join(tit_eleitor_str.split())\n",
    "    tit_eleitor_str = tit_eleitor_str.rjust(12,'0')\n",
    "    return tit_eleitor_str\n",
    "\n",
    "def clean_cpf(cpf_series):\n",
    "    \"\"\"\n",
    "    Apply hard-coded translation of CPFs to a Pandas Series `cpf_series`.\n",
    "    The hard-coded dict was created based on the search for other entries \n",
    "    (in other years) with same name and Tit. eleitor.\n",
    "    \"\"\"\n",
    "    cpf_fixer = {'48 110000':'53909542549', '60 30434549':'60630434549', '58 753000':'68411537404'}\n",
    "    cpf_series = cpf_series.map(xu.translate_dict(cpf_fixer))\n",
    "    return cpf_series\n",
    "\n",
    "\n",
    "def clean_votos_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"votacao_candidato_munzona\" dataframes.\n",
    "    \"\"\"\n",
    "    \n",
    "    # SG_UE for 2006 is wrong, fix it:\n",
    "    df.loc[df['ANO_ELEICAO'] == 2006, 'SG_UE'] = df.loc[df['ANO_ELEICAO'] == 2006, 'SG_UF']\n",
    "    \n",
    "    # Standardize nome do cargo:\n",
    "    df['DS_CARGO'] = df['DS_CARGO'].str.upper()\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_cand_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"consulta_cand\" DataFrame.\n",
    "    \"\"\"\n",
    "    # Fix birth dates manually:\n",
    "    bad_date = ['3010196', '3010179', '2801 966', '29021954', '29021947']\n",
    "    fix_date = ['30101960', '30101979', '28011966', '28021954', '28021947']\n",
    "    for bad, fix in zip(bad_date, fix_date):\n",
    "        df.loc[df['DT_NASCIMENTO'] == bad, 'DT_NASCIMENTO'] = fix\n",
    "    # Fix birth dates with formula:\n",
    "    df['DT_NASCIMENTO'] = df['DT_NASCIMENTO'].apply(format_date)\n",
    "\n",
    "    # Fix level of education:\n",
    "    df['DS_GRAU_INSTRUCAO'] = df['DS_GRAU_INSTRUCAO'].str.replace('^FUNDAMENTAL', 'ENSINO FUNDAMENTAL', regex=True)\n",
    "    df['DS_GRAU_INSTRUCAO'] = df['DS_GRAU_INSTRUCAO'].str.replace('^MÉDIO', 'ENSINO MÉDIO', regex=True)\n",
    "\n",
    "    # Clean documents (CPF and Tit Eleitor):\n",
    "    df['NR_TITULO_ELEITORAL_CANDIDATO'] = df['NR_TITULO_ELEITORAL_CANDIDATO'].apply(clean_tit_eleitor)\n",
    "    df['NR_CPF_CANDIDATO'] = clean_cpf(df['NR_CPF_CANDIDATO'])\n",
    "    \n",
    "    # Standardize descrição do cargo:\n",
    "    df['DS_CARGO'] = df['DS_CARGO'].str.replace('º SUPLENTE SENADOR', 'º SUPLENTE')\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_bem_df(df):\n",
    "    \"\"\"\n",
    "    Clean and fix \"bem_candidato\" DataFrame.\n",
    "    \"\"\"\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].str.replace(',', '.')\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].astype(float)\n",
    "    # Some values are negative, but by the looks of it, this is a mistake:\n",
    "    df['VR_BEM_CANDIDATO'] = df['VR_BEM_CANDIDATO'].abs()\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_header_from_file(filename):\n",
    "    \"\"\"\n",
    "    Load the header associated with a single TSE data file `filename` (str).\n",
    "    \"\"\"\n",
    "    return load_tse_file(filename).columns\n",
    "\n",
    "\n",
    "def load_header_from_dir(folder):\n",
    "    \"\"\"\n",
    "    Load the header associated with a folder of TSE data files `folder` (str).\n",
    "    \"\"\"\n",
    "    return load_tse_dir(folder).columns\n",
    "\n",
    "\n",
    "def clear_all_null_tokens(df):\n",
    "    \"\"\"\n",
    "    Replace all known TSE tokens that represent missing values with None.\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        df.loc[df[col].isin([-1, -3, '-1', '-3', '#NULO#', '#NE#', '#NI#', '00000000#NI#', '0000000000-1']), col] = None\n",
    "    return df\n",
    "\n",
    "\n",
    "def etl_cand(folder, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidates (consulta_cand).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    folder : str\n",
    "        Path to the folder containing the data. For data files that do not contain headers\n",
    "        (.txt files), the folder should also contain a 'header.txt' file with the header \n",
    "        in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a single file that will receive all the data in `folder`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    consulta_cand_old_cols = ['IDADE_DATA_ELEICAO', 'SG_LEGENDA']\n",
    "    # How to read in certain columns:\n",
    "    cand_types = {'DATA_NASCIMENTO':str, 'CPF_CANDIDATO':str, 'NUM_TITULO_ELEITORAL_CANDIDATO':str,\n",
    "                  'NR_TITULO_ELEITORAL_CANDIDATO': str, 'NR_CPF_CANDIDATO': str}    \n",
    "    \n",
    "    # Load data from folder:\n",
    "    df = load_tse_dir(folder, cand_types, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    trans = standardize_df_columns(df, template_header, consulta_cand_old_cols)\n",
    "    trans = clean_cand_df(trans)\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "\n",
    "def etl_bem(folder, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidates' wealth (bem_candidato).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    folder : str\n",
    "        Path to the folder containing the data. For data files that do not contain headers\n",
    "        (.txt files), the folder should also contain a 'header.txt' file with the header \n",
    "        in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a single file that will receive all the data in `folder`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    bem_candidato_old_cols = []\n",
    "    # How to read in certain columns:\n",
    "    bem_types  = {'VALOR_BEM':str, 'VR_BEM_CANDIDATO':str}\n",
    "    \n",
    "    # Load data from folder:\n",
    "    print('Load data...')\n",
    "    df = load_tse_dir(folder, bem_types, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    print('Standardize...')\n",
    "    trans = standardize_df_columns(df, template_header, bem_candidato_old_cols)\n",
    "    print('Clean...')\n",
    "    trans = clean_bem_df(trans)\n",
    "    print('Clear nulls...')\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    print('Save file...')\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)\n",
    "\n",
    "\n",
    "def etl_votos(infile, template_header, destination):\n",
    "    \"\"\"\n",
    "    Extract, Transform and Load TSE data about the candidate's votes (votacao_candidato_munzona).\n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    infile : str\n",
    "        Path to the data file. For data files that do not contain headers\n",
    "        (.txt files), the folder containing the file should also contain a 'header.txt' \n",
    "        file with the header in the same format as the data files.\n",
    "        \n",
    "    template_header : list of str\n",
    "        The column names that come from a template. Those are most of the columns that \n",
    "        will appear in the output file (some others are hard-coded).\n",
    "    \n",
    "    destination : str\n",
    "        Path to a file that will receive all the  cleaned data from `infile`.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded stuff:\n",
    "    # Colunas extras (para além do template): \n",
    "    votacao_old_cols = []\n",
    "    \n",
    "    # Load data from folder:\n",
    "    df = load_tse_file(infile, verbose=False)\n",
    "    df_header = df.columns\n",
    "    \n",
    "    # Standardize CONSULTA CAND:\n",
    "    trans = standardize_df_columns(df, template_header, votacao_old_cols)\n",
    "    trans = clean_votos_df(trans)\n",
    "    trans = clear_all_null_tokens(trans)\n",
    "    \n",
    "    # Save file:\n",
    "    xu.make_necessary_dirs(destination)\n",
    "    trans.to_csv(destination, index=False, quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpa dados do TSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidatos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cand_header = load_header_from_dir('/home/skems/ceweb/dados/brutos/tse/consulta_cand_2022/')\n",
    "etl_cand('/home/skems/ceweb/dados/brutos/tse/consulta_cand_2022/', cand_header, '/home/skems/ceweb/dados/limpos/tse/consulta_cand/consulta_cand_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n",
      "Standardize...\n",
      "Clean...\n",
      "Clear nulls...\n",
      "Save file...\n"
     ]
    }
   ],
   "source": [
    "bem_header = load_header_from_dir('/home/skems/ceweb/dados/brutos/tse/bem_candidato_2022/')\n",
    "etl_bem('/home/skems/ceweb/dados/brutos/tse/bem_candidato_2022/', bem_header, '/home/skems/ceweb/dados/limpos/tse/bens_candidatos/bem_candidato_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xavy.explore as xe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bens_df = pd.read_csv('/home/skems/ceweb/dados/limpos/tse/bens_candidatos/bem_candidato_2022_BRASIL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "Empty DataFrame\n",
      "Columns: [coluna, N, %]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "xe.checkMissing(bens_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1mDT_GERACAO: \u001b[0m1 unique values.\n",
      "16/08/2022\n",
      "\n",
      "\u001b[1mHH_GERACAO: \u001b[0m1 unique values.\n",
      "06:07:10\n",
      "\n",
      "\u001b[1mANO_ELEICAO: \u001b[0m1 unique values.\n",
      "2022\n",
      "\n",
      "\u001b[1mCD_TIPO_ELEICAO: \u001b[0m1 unique values.\n",
      "2\n",
      "\n",
      "\u001b[1mNM_TIPO_ELEICAO: \u001b[0m1 unique values.\n",
      "Eleição Ordinária\n",
      "\n",
      "\u001b[1mCD_ELEICAO: \u001b[0m2 unique values.\n",
      "544,  546\n",
      "\n",
      "\u001b[1mDS_ELEICAO: \u001b[0m2 unique values.\n",
      "Eleição Geral Federal 2022,  Eleições Gerais Estaduais 2022\n",
      "\n",
      "\u001b[1mDT_ELEICAO: \u001b[0m1 unique values.\n",
      "02/10/2022\n",
      "\n",
      "\u001b[1mSG_UF: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mAL,  BA,  BR,  CE,  DF,  ES,  GO,  MA,  MG,  MS,  MT,  PA,  PB,  PE,  PR,  RR,  RS,  SE,  SP,  TO\n",
      "\n",
      "\u001b[1mSG_UE: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mAL,  AM,  BA,  BR,  CE,  ES,  GO,  MA,  MS,  MT,  PA,  PB,  PE,  PI,  RJ,  RS,  SC,  SE,  SP,  TO\n",
      "\n",
      "\u001b[1mNM_UE: \u001b[0m28 unique values.\n",
      "\u001b[1m(sample) \u001b[0mACRE,  ALAGOAS,  AMAPÁ,  AMAZONAS,  BRASIL,  DISTRITO FEDERAL,  ESPÍRITO SANTO,  GOIÁS,  MINAS GERAIS,  PARANÁ,  PARAÍBA,  PERNAMBUCO,  PIAUÍ,  RIO DE JANEIRO,  RIO GRANDE DO NORTE,  RIO GRANDE DO SUL,  RORAIMA,  SANTA CATARINA,  SÃO PAULO,  TOCANTINS\n",
      "\n",
      "\u001b[1mSQ_CANDIDATO: \u001b[0m17657 unique values.\n",
      "\u001b[1m(sample) \u001b[0m50001604575,  50001605371,  50001615392,  50001648305,  60001601193,  60001619104,  60001645845,  70001600886,  70001609159,  110001604914,  110001719539,  120001652310,  130001606748,  160001621908,  190001601408,  210001601598,  210001614090,  250001611410,  250001612276,  250001613776\n",
      "\n",
      "\u001b[1mNR_ORDEM_CANDIDATO: \u001b[0m180 unique values.\n",
      "\u001b[1m(sample) \u001b[0m17,  28,  52,  56,  60,  68,  74,  77,  84,  87,  98,  104,  111,  116,  120,  122,  123,  128,  145,  180\n",
      "\n",
      "\u001b[1mCD_TIPO_BEM_CANDIDATO: \u001b[0m50 unique values.\n",
      "\u001b[1m(sample) \u001b[0m2,  11,  13,  14,  22,  23,  29,  39,  41,  47,  52,  53,  54,  69,  71,  73,  74,  91,  96,  99\n",
      "\n",
      "\u001b[1mDS_TIPO_BEM_CANDIDATO: \u001b[0m50 unique values.\n",
      "\u001b[1m(sample) \u001b[0mAeronave,  Apartamento,  Benfeitorias,  Crédito decorrente de alienação,  Crédito decorrente de empréstimo,  Depósito bancário em conta corrente no exterior,  Direito de autor, de inventor e patente,  Embarcação,  Fundo de Curto Prazo,  Fundo de Investimento Imobiliário,  Fundos: Ações, Mútuos de Privatização, Invest. Empresas Emergentes, Invest.Participação e Invest. Índice Mercado,  Leasing,  Licença e concessões especiais,  Ouro, ativo financeiro,  Outras aplicações e Investimentos,  Outros depósitos à vista e numerário,  Prédio residencial,  Quotas ou quinhões de capital,  Título de clube e assemelhado,  Veículo automotor terrestre: caminhão, automóvel, moto, etc.\n",
      "\n",
      "\u001b[1mDS_BEM_CANDIDATO: \u001b[0m1 unique values.\n",
      "Não divulgável\n",
      "\n",
      "\u001b[1mVR_BEM_CANDIDATO: \u001b[0m36902 unique values.\n",
      "\u001b[1m(sample) \u001b[0m8.58,  90.55,  483.69,  1345.5,  1575.26,  1676.68,  3545.04,  5129.85,  6094.49,  10740.5,  14061.66,  22685.24,  23975.81,  29219.4,  53464.6,  62448.0,  77226.78,  92657.4,  125990.0,  194571.0\n",
      "\n",
      "\u001b[1mDT_ULTIMA_ATUALIZACAO: \u001b[0m21 unique values.\n",
      "\u001b[1m(sample) \u001b[0m01/08/2022,  02/08/2022,  03/08/2022,  04/08/2022,  05/08/2022,  07/08/2022,  08/08/2022,  09/08/2022,  10/08/2022,  11/08/2022,  12/08/2022,  13/08/2022,  14/08/2022,  15/08/2022,  26/07/2022,  27/07/2022,  28/07/2022,  29/07/2022,  30/07/2022,  31/07/2022\n",
      "\n",
      "\u001b[1mHH_ULTIMA_ATUALIZACAO: \u001b[0m13287 unique values.\n",
      "\u001b[1m(sample) \u001b[0m10:47:46,  11:01:01,  11:42:15,  12:06:44,  12:37:02,  12:52:59,  12:55:57,  13:18:34,  13:46:20,  14:02:05,  14:02:50,  14:45:05,  15:11:57,  15:47:39,  16:02:28,  16:05:00,  16:43:08,  17:18:41,  18:18:26,  19:03:56\n"
     ]
    }
   ],
   "source": [
    "xe.mapUnique(bens_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros dados (código antigo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "AC AL AM AP BA CE ES GO MA MG MS MT PA PB PE PI PR RJ RN RO RR RS SC SE SP TO \n"
     ]
    }
   ],
   "source": [
    "# ETL votos:\n",
    "\n",
    "header_file  = '../dados/brutos/votacao_candidato_munzona_2018/votacao_candidato_munzona_2018_AC.csv'\n",
    "votos_header = load_header_from_file(header_file)\n",
    "\n",
    "for year in range(2004, 2022, 2):\n",
    "    print(year)\n",
    "    file_list = sorted(glob('../dados/brutos/votacao_candidato_munzona_' + str(year) + '/votacao_candidato_munzona_' + str(year) + '_??.*'))\n",
    "    for f in file_list:\n",
    "        uf = f[-6:-4]\n",
    "        print(uf, end=' ')\n",
    "        \n",
    "        outfile = '../dados/limpos/votacao_candidato_munzona/' + f.split('/')[-1].replace('.txt', '.csv')\n",
    "        etl_votos(f, votos_header, outfile)\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "Load data...\n",
      "Standatdize...\n",
      "Clean...\n",
      "Clear nulls...\n",
      "Save file...\n"
     ]
    }
   ],
   "source": [
    "# ETL bens dos candidatos:\n",
    "\n",
    "bem_header = load_header_from_dir('../dados/brutos/bem_candidato_2020/')\n",
    "\n",
    "for year in range(2006, 2022, 4):\n",
    "    print(year)\n",
    "    in_folder = '../dados/brutos/bem_candidato_' + str(year) + '/'\n",
    "    out_file  = '../dados/limpos/bem_candidato/bem_candidato_' + str(year) + '_BRASIL.csv'\n",
    "    etl_bem(in_folder, bem_header, out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
