{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8835758c-f44f-4c16-ae76-d74b57ee85f6",
   "metadata": {},
   "source": [
    "# Capturando tweets direcionados a candidatos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d89b4-23d6-4ce8-897f-f04487d7d158",
   "metadata": {},
   "source": [
    "[Voltar ao Índice](00_indice.ipynb)\n",
    "\n",
    "**ATTENTION:** This notebook was used as a sandbox for prototyping and testing code. It is not expected to work, and is only provided for historical reasons and purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f713b-f7fd-4624-a016-8ff92fd2cb05",
   "metadata": {},
   "source": [
    "**FAZER**\n",
    "* Lidar com o caso de mais de um twitter por candidato (tanto de mais de uma linha na tabela **Feito** quanto de mais de um usuário listado na mesma linha).\n",
    "* Selecionar apenas tweets direcionados a uma única pessoa; **Feito**\n",
    "* Identificar violência direcionada a outras pessoas que não o mencionado. **Feito**\n",
    "* Ignorar tweets que não contém texto. **Feito**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54c3845c-d4ac-4971-ba03-37185e06ea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as pl\n",
    "import json\n",
    "import traceback\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "import src.xavy.twitter as tw\n",
    "import src.xavy.explore as xe\n",
    "import src.xavy.dataframes as xd\n",
    "#import speechwrapper as sw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6766cf22-0b65-45d0-a114-35ac27e478d1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b236c1d5-091c-424c-ba37-c5e3228f0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_twitter_username(series, lower=True):\n",
    "    \"\"\"\n",
    "    Given a Series that contains the URL \n",
    "    or the username of twitter accounts,\n",
    "    extract the username.\n",
    "    \n",
    "    If `lower` is True, return lower case\n",
    "    username (Twitter usernames are not \n",
    "    case-sensitive).\n",
    "    \n",
    "    Returns a series with same index as \n",
    "    the input. NaN is returned when no\n",
    "    unsername is found.\n",
    "    \"\"\"\n",
    "    \n",
    "    username =  series.str.extract('(?:[Tt]wit+er\\.com(?:\\.br)?/@?|@)(\\w+)')[0]\n",
    "    if lower is True:\n",
    "        username = username.str.lower()\n",
    "    \n",
    "    return username\n",
    "\n",
    "def request_twitter_user_info(df, username_col='twitter_username', cand_id_col='SQ_CANDIDATO', requests_per_window=900):\n",
    "    \"\"\"\n",
    "    Collect twitter user info using Twitter\n",
    "    API v.1.1.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Table containing the twitter username \n",
    "        and a candidate identifier (e.g. \n",
    "        'SQ_CANDIDATO').\n",
    "    username_col : str\n",
    "        Name of the column containing the \n",
    "        twitter usernames.\n",
    "    cand_id_col : str\n",
    "        Column identifying the candidate.\n",
    "    requests_per_window : int\n",
    "        Maximum number of requests allowed\n",
    "        by the API in a 15-minute window.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    response_df : DataFrame\n",
    "        DataFrame with all the information\n",
    "        provided by the API, for each \n",
    "        username found, along with the \n",
    "        associated candidate ID.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Look for twitter IDs with API:\n",
    "    w_username = df[username_col].dropna().drop_duplicates()\n",
    "    response = tw.lookup(w_username, requests_per_window=requests_per_window)\n",
    "    \n",
    "    # Build DataFrame with responses:\n",
    "    response_df = pd.DataFrame(response['data'])\n",
    "    # Join SQ_CANDIDATO to twitter data:\n",
    "    response_df['lower_name'] = response_df['username'].str.lower()\n",
    "    cand_ids = df.set_index(username_col)[cand_id_col].astype(str)\n",
    "    response_df = response_df.join(cand_ids, on='lower_name')\n",
    "    \n",
    "    # Expand dict Series (nested data):\n",
    "    for col in response_df.columns:\n",
    "        if type(response_df[col].iloc[0]) is dict:\n",
    "            nested_df = pd.DataFrame(list(response_df[col]), index=response_df.index)\n",
    "            response_df = response_df.join(nested_df)\n",
    "            response_df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return response_df\n",
    "\n",
    "def append_mentions_page(mentions, url, parameters):\n",
    "    \"\"\"\n",
    "    Update `mentions` and `parameters` in place\n",
    "    by appending twitter API /2/users/:id/mentions\n",
    "    responses and getting next page token.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mentions : dict\n",
    "        Concatenated data from multiple API\n",
    "        calls.\n",
    "    url : str\n",
    "        Twitter API URL \n",
    "        ('https://api.twitter.com/2/users/:id/mentions')\n",
    "    parameters : dict\n",
    "        API call parameters, which may include a \n",
    "        'pagination_token' key. The latter is \n",
    "        updated in place if there is a 'next_page' \n",
    "        in the API response.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    appended_mentions : dict\n",
    "        Dict containing the data from `mentions`, \n",
    "        appended by the data from the API call.\n",
    "    is_new_page : bool\n",
    "        True if there is another page after\n",
    "        the current one, and False otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    extra = tw.request_twitter_api(url, parameters)\n",
    "    mentions = tw.concat_json_pages(mentions, extra)\n",
    "    if 'next_token' in extra['meta']:\n",
    "        parameters.update({'pagination_token': extra['meta']['next_token']})\n",
    "        return mentions, True\n",
    "    \n",
    "    return mentions, False\n",
    "\n",
    "def mentions_to_df(mentions, user_id):\n",
    "    \"\"\"\n",
    "    Parse JSON structure containing twitter \n",
    "    mentions to a user into a DataFrame.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mentions : dict\n",
    "        Twitter API response to \n",
    "        /2/users/:id/mentions endpoint.\n",
    "    user_id : str ir int\n",
    "        Twitter ID of the mentioned user.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mentions_df : DataFrame\n",
    "        Data from the API parsed into a \n",
    "        DataFrame, with some extra columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cria DataFrame de tweets mencionando usuário:\n",
    "    mentions_df = pd.DataFrame(mentions['data'])\n",
    "    n_mentions  = len(mentions_df)\n",
    "    \n",
    "    # Adiciona coluna 'in reply...' se não existir (acho que isso acontece quando nenhuma das menções é reply):\n",
    "    if 'in_reply_to_user_id' not in mentions_df.columns:\n",
    "        mentions_df['in_reply_to_user_id'] = np.NaN\n",
    "    \n",
    "    # Junta informações sobre o autor da menção:\n",
    "    participants_df = pd.DataFrame(mentions['includes']['users'])[['id', 'name', 'username']].drop_duplicates()\n",
    "    author_fields = {'name':'author_name', 'username':'author_username'}\n",
    "    mentions_df = mentions_df.join(participants_df.rename(author_fields, axis=1).set_index('id'), on='author_id')\n",
    "    assert(len(mentions_df) == n_mentions), 'Author info join increased number of mentions. This is wrong.'\n",
    "    \n",
    "    # Adiciona link p/ o tweet:\n",
    "    mentions_df['tweet_url'] = 'https://www.twitter.com/' + mentions_df['author_username'] + '/status/' + mentions_df['id'].astype(str)\n",
    "    \n",
    "    # Parseia data (se existir):\n",
    "    if 'created_at' in mentions_df.columns:\n",
    "        mentions_df['created_at'] = pd.to_datetime(mentions_df['created_at'])\n",
    "    \n",
    "    # Conta número de usuários mencionados:\n",
    "    if 'entities' in mentions_df.columns:\n",
    "        mentions_df['n_mentions'] = mentions_df['entities'].apply(lambda s: len(s['mentions']))\n",
    "    \n",
    "    # Identifica reply direto:\n",
    "    mentions_df['direct_reply'] = (mentions_df['in_reply_to_user_id'] == str(user_id)).astype(int)\n",
    "    \n",
    "    # Info da captura:\n",
    "    \n",
    "    \n",
    "    return mentions_df\n",
    "\n",
    "def parse_utc_time(time_in, time_fmt='%Y-%m-%dT%H:%M:%S', bsb2utc=True):\n",
    "    \"\"\"\n",
    "    Parse a (possibly) local time into UTC time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    time_in : str or datetime\n",
    "        Time to parse to UTC datetime.\n",
    "    time_fmt : str\n",
    "        If `time_in` is str, this is used to parse it to datetime.\n",
    "    bsb2utc : bool\n",
    "        Whether to assume `time_in` is Brasilia local time and \n",
    "        convert it to UTC.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    time_utc : datetime\n",
    "        Time in UTC (assuming `time_in` is UTC and `bsb2utc` is\n",
    "        False; or `time_in` is UTC-3 and `bsb2utc` is True).\n",
    "    \"\"\"\n",
    "    # Parse str to datetime:\n",
    "    if type(time_in) is str:\n",
    "        time_dt = dt.datetime.strptime(time_in, time_fmt)\n",
    "    else:\n",
    "        time_dt = time_in\n",
    "        \n",
    "    # Convert Brazilia (UTC-3) to UTC:\n",
    "    if bsb2utc is True:\n",
    "        time_dt = time_dt + dt.timedelta(hours=3)\n",
    "    \n",
    "    return time_dt\n",
    "\n",
    "def get_mentions_in_period(user_id, start_time, end_time, max_pages=20, max_results=100, requests_per_window=450, verbose=True, bsb_time=True):\n",
    "    \"\"\"\n",
    "    Get mentions to specified user within a period of time.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_id : int\n",
    "        Twitter user ID to check mentions for.\n",
    "    start_time : str or datetime\n",
    "        Beginning of the period in which to look for mentions. \n",
    "        If str, must be in the format ''%Y-%m-%dT%H:%M:%S'.\n",
    "    end_time : str or datetime\n",
    "        End of the period in which to look for mentions. \n",
    "        If str, must be in the format ''%Y-%m-%dT%H:%M:%S'.\n",
    "    max_pages : int\n",
    "        Maximum number of pages to go through when a paginated\n",
    "        result is returned. Note that the API only checks and \n",
    "        returns the 800 most recent tweets.\n",
    "    max_results : int\n",
    "        Maximum number of results to return in each API call, \n",
    "        that is, in each page.\n",
    "    requests_per_window : int\n",
    "        Maximum number of calls in a 15-minute window allowed \n",
    "        by the API. Each call for a page is delayed by the \n",
    "        appropriate amount of time to avoid reaching this \n",
    "        limit.\n",
    "    verbose : bool\n",
    "        Whether to print page numbers as going through the\n",
    "        pagination.\n",
    "    bsb_time : bool\n",
    "        Whether `start_time` and `end_time` are given at \n",
    "        Brasilia local time (UTC-3).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mentions : dict\n",
    "        The API response, containing the tweets mentioning the \n",
    "        user `user_id`, after concatenating the pages.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Hard-coded:\n",
    "    url_template = 'https://api.twitter.com/2/users/{}/mentions'\n",
    "    params = {'tweet.fields': ['created_at'], 'expansions':['author_id', 'in_reply_to_user_id', 'entities.mentions.username']}\n",
    "    \n",
    "    # Prepate input:\n",
    "    sleep_time = tw.compute_sleep_time(requests_per_window)\n",
    "    url = url_template.format(user_id)\n",
    "    start_utc = parse_utc_time(start_time, bsb2utc=bsb_time).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    end_utc   = parse_utc_time(end_time, bsb2utc=bsb_time).strftime('%Y-%m-%dT%H:%M:%SZ')\n",
    "    params.update({'max_results': max_results, 'start_time': start_utc, 'end_time': end_utc})\n",
    "    \n",
    "    mentions = {}\n",
    "    # First capture:\n",
    "    time.sleep(sleep_time)\n",
    "    mentions, get_next_page = append_mentions_page(mentions, url, params)\n",
    "    page_num = 1\n",
    "    if verbose is True:\n",
    "        print(page_num, end=' ')\n",
    "    # Go through pagination:\n",
    "    while get_next_page is True and page_num < max_pages:\n",
    "        time.sleep(sleep_time)\n",
    "        mentions, get_next_page = append_mentions_page(mentions, url, params)\n",
    "        page_num += 1\n",
    "        if verbose is True:\n",
    "            print(page_num, end=' ')\n",
    "    \n",
    "    return mentions\n",
    "\n",
    "def compute_time_period(start_time, end_time, time_fmt='%Y-%m-%dT%H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Return the time interval in hours (float) between the\n",
    "    `start_time` and `end_time` (both str or datetime).\n",
    "    If str, the input should be provided in the `time_fmt`\n",
    "    format.\n",
    "    \"\"\"\n",
    "    return (parse_utc_time(end_time, time_fmt) - parse_utc_time(start_time, time_fmt)).total_seconds() / 3600\n",
    "\n",
    "def capture_stats(mentions, start_time, end_time, time_fmt='%Y-%m-%dT%H:%M:%S', max_mentions=800):\n",
    "    \"\"\"\n",
    "    Compute statistical information about the\n",
    "    response of an API mentions request.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mentions : dict\n",
    "        Response from an API call, as returned by \n",
    "        the `get_mentions_in_period` function.\n",
    "    start_time : str or datetime\n",
    "        Start of the time period requested for the\n",
    "        capture with `get_mentions_in_period`.\n",
    "    end_time : str or datetime\n",
    "        End of the time period requested for the\n",
    "        capture with `get_mentions_in_period`.\n",
    "    time_fmt : str\n",
    "        Format of `start_time` and `end_time`.\n",
    "    max_mentions : int\n",
    "        Maximum number of recent tweets returned by\n",
    "        the API. NOTE THAT THE CAPTURE END TIME \n",
    "        SHOULD BE THE CURRENT TIME FOR THE STATISTICS\n",
    "        TO BE RIGHT.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    n_mentions : int\n",
    "    n_errors : int\n",
    "    time_window : float\n",
    "    collected_time : float\n",
    "    t_win_weight : float\n",
    "    \"\"\"\n",
    "    \n",
    "    # Request stats:\n",
    "    n_mentions  = np.sum(mentions['meta']['result_count'])\n",
    "    if 'errors' not in mentions.keys():\n",
    "        n_errors = 0\n",
    "    else:\n",
    "        n_errors    = len(mentions['errors'])\n",
    "\n",
    "    # The time period expected to be covered by the request:\n",
    "    time_window = compute_time_period(start_time, end_time, time_fmt)\n",
    "    # Actual time period covered:\n",
    "    collected_time = compute_time_period(mentions['data'][-1]['created_at'], mentions['data'][0]['created_at'], time_fmt='%Y-%m-%dT%H:%M:%S.000Z')\n",
    "    # Compute statistical weight of the tweet to represent the expected time period:\n",
    "    if n_mentions >= max_mentions:\n",
    "        t_win_weight = time_window / collected_time\n",
    "    else:\n",
    "        t_win_weight = 1.0\n",
    "    \n",
    "    return n_mentions, n_errors, time_window, collected_time, t_win_weight\n",
    "\n",
    "def get_last_mentions(user_id, last_hours=6, verbose=True):\n",
    "    \"\"\"\n",
    "    Capture mentions to a twitter user in the last couple of hours.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    user_id : int\n",
    "        Twitter user ID to look mentions for.\n",
    "    last_hours : float\n",
    "        Number of hours in the past, from current time, to look \n",
    "        for mentions. Remember that the API returns at most \n",
    "        800 most recent tweets.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mentions_df : DataFrame\n",
    "        Table containing the tweets mentioning `user_id` in the \n",
    "        `last_hours`, along with the author info and capture \n",
    "        process stats.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set time landmarks:\n",
    "    end_time   = dt.datetime.now()\n",
    "    start_time = end_time - dt.timedelta(hours=last_hours)\n",
    "    \n",
    "    # Capture the mentions:\n",
    "    mentions = get_mentions_in_period(user_id, start_time, end_time, verbose=verbose)\n",
    "    \n",
    "    # Exit if there is no data:\n",
    "    if mentions['meta']['result_count'] == 0:\n",
    "        return None, {'batch_start': start_time, 'batch_end': end_time, 'batch_tweets': 0, 'batch_errors': 0}\n",
    "    \n",
    "    # Build the DataFrame:\n",
    "    m_df = mentions_to_df(mentions, user_id)\n",
    "    m_df['batch_user']  = user_id\n",
    "    m_df['batch_start'] = start_time\n",
    "    m_df['batch_end']   = end_time\n",
    "    m_df['batch_tweets'], m_df['batch_errors'], m_df['target_t_win'], m_df['actual_t_win'], m_df['t_win_weight'] = capture_stats(mentions, start_time, end_time)\n",
    "    \n",
    "    return m_df, {'batch_start': start_time, 'batch_end': end_time, 'batch_tweets': m_df.iloc[0]['batch_tweets'], 'batch_errors': m_df.iloc[0]['batch_errors']}\n",
    "\n",
    "def todays_tweet_limit(curr_level, cap_renew_date, tweet_cap=2000000, safety_buffer=100):\n",
    "    \"\"\"\n",
    "    Compute maximum number of tweets that should be captured\n",
    "    per day given the current capture quota usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    curr_level : int\n",
    "        Number of tweets already captured.\n",
    "    cap_renew_date : str\n",
    "        Date when the usage cap resets, in format '%Y-%m-%d'.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    tweet_cap : int\n",
    "        Monthly tweet cap.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    safety_buffer : int\n",
    "        Decrement in the number of tweets to be captured per pay,\n",
    "        to avoid errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    todays_lim : int\n",
    "        Maximum number of tweets that should be captured today,\n",
    "        assuming a constant rate up to cap renew date.\n",
    "    \"\"\"\n",
    "    today = dt.date.today()\n",
    "    renew = dt.date(*(int(x) for x in cap_renew_date.split('-')))\n",
    "    days_to_renew = (renew - today).days\n",
    "    if days_to_renew <=0:\n",
    "        raise Exception('{} reached cap renew date {}: reset `cap_renew_date` to new date.'.format(today, renew))\n",
    "    todays_lim    = int((tweet_cap - curr_level) / days_to_renew - safety_buffer)\n",
    "    \n",
    "    return todays_lim\n",
    "\n",
    "def todays_n_cands(curr_level, cap_renew_date, avg_tweets, tweet_cap=2000000, tweets_buffer=100):\n",
    "    \"\"\"\n",
    "    Compute the number of candidates to look mentions for in the\n",
    "    period of one day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    curr_level : int\n",
    "        Number of tweets already captured.\n",
    "    cap_renew_date : str\n",
    "        Date when the usage cap resets, in format '%Y-%m-%d'.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    avg_tweets : float\n",
    "        Average number of tweets mentioning a candidate in the \n",
    "        capture period.\n",
    "    tweet_cap : int\n",
    "        Monthly tweet cap.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    tweets_buffer : int\n",
    "        Decrement in the number of tweets to be captured per pay,\n",
    "        to avoid errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    n_cands : int\n",
    "        Number of candidates that can have their mentions captured.\n",
    "    \"\"\"\n",
    "    \n",
    "    return int(todays_tweet_limit(curr_level, cap_renew_date, tweet_cap, tweets_buffer) / avg_tweets)\n",
    "\n",
    "def batch_n_cands(curr_level, cap_renew_date, avg_tweets, capture_period, tweet_cap=2000000, tweets_buffer=100):\n",
    "    \"\"\"\n",
    "    Compute the number of candidates to look mentions for in \n",
    "    a capture batch.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    curr_level : int\n",
    "        Number of tweets already captured.\n",
    "    cap_renew_date : str\n",
    "        Date when the usage cap resets, in format '%Y-%m-%d'.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    avg_tweets : float\n",
    "        Average number of tweets mentioning a candidate in the \n",
    "        capture period.\n",
    "    capture_period : int\n",
    "        Capture window size for each user, in hours. \n",
    "    tweet_cap : int\n",
    "        Monthly tweet cap.\n",
    "        Check https://developer.twitter.com/en/portal/dashboard.\n",
    "    tweets_buffer : int\n",
    "        Decrement in the number of tweets to be captured per pay,\n",
    "        to avoid errors.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    n_cands : int\n",
    "        Number of candidates that can have their mentions captured.\n",
    "    \"\"\"\n",
    "    \n",
    "    day_n_cands   = todays_n_cands(curr_level, cap_renew_date, avg_tweets, tweet_cap, tweets_buffer)\n",
    "    batch_n_cands = int(day_n_cands / (24 / capture_period))\n",
    "    \n",
    "    return batch_n_cands\n",
    "\n",
    "def read_config(filename='tweet_capture_config.json'):\n",
    "    \"\"\"\n",
    "    Read JSON from `filename` (str).\n",
    "    \"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    return config\n",
    "\n",
    "def write_config(config, filename='tweet_capture_config.json'):\n",
    "    \"\"\"\n",
    "    Write `config` (dict) to `filename` (str) as JSON.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(config, f, indent=1)\n",
    "\n",
    "def program_batch_capture(twitter_df, n_cands, previous_df=None, start_time=None, time_fmt='%Y-%m-%dT%H:%M:%S', random_state=None):\n",
    "    \"\"\"\n",
    "    Generate DataFrame with a schedule for capturing data from \n",
    "    randomly sampled candidates, organized in batches spaced \n",
    "    during the day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    twitter_df : DataFrame\n",
    "        Table with column 'id' containing all the candidates'\n",
    "        Twitter IDs.\n",
    "    n_cands : int\n",
    "        Number of candidates to randomly select for today's \n",
    "        capture.\n",
    "    previous_df : DataFrame\n",
    "        Capture list from the previous batch. These user IDs \n",
    "        sre removed from the set before sampling, to avoid \n",
    "        data overlap.\n",
    "    start_time : str, datetime or None\n",
    "        Datetime to schedule the capture to. If str, in format \n",
    "        given by `time_fmt`. If None, get the current date.\n",
    "    time_fmt : str\n",
    "        Format of `start_time`, if provided as str.\n",
    "    random_state : int or None\n",
    "        Seed for randomly selecting candidates. Use None for \n",
    "        random seed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Table with candidates to capture, their batches and \n",
    "        the time they should be captured.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current date if needed:\n",
    "    if start_time is None:\n",
    "        start_time = dt.datetime.now()\n",
    "    else: \n",
    "        start_time = parse_utc_time(start_time, time_fmt, bsb2utc=False)\n",
    "        \n",
    "    # Select not repeating sample:\n",
    "    if previous_df is None:\n",
    "        no_repeat = twitter_df\n",
    "    else:\n",
    "        no_repeat = twitter_df.loc[~twitter_df['id'].isin(previous_df['id'])]\n",
    "    \n",
    "    # Randomly select candidates:\n",
    "    daily_capture_df = no_repeat['id'].sample(n_cands, random_state=random_state).reset_index()\n",
    "    daily_capture_df.rename({'index':'cand_id_pos'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Prepare batch information:\n",
    "    daily_capture_df['batch_size'] = n_cands\n",
    "    daily_capture_df['batch_time'] = start_time\n",
    "    daily_capture_df['status'] = 'queue'\n",
    "    daily_capture_df['batch_start'] = np.NaN\n",
    "    daily_capture_df['batch_end'] = np.NaN\n",
    "    daily_capture_df['batch_tweets'] = np.NaN\n",
    "    daily_capture_df['batch_errors'] = np.NaN\n",
    "    \n",
    "    return daily_capture_df\n",
    "\n",
    "def print_to_file(error_log, filename):\n",
    "    \"\"\"\n",
    "    Print `error_log` (str) into file with `filename` (str).\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(error_log)\n",
    "\n",
    "\n",
    "def gen_mentions_path(data_dir, batch_time, user_id):\n",
    "    \"\"\"\n",
    "    Create filename and path for storing the data obtained \n",
    "    from a mentions capture.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir : str\n",
    "        Path to the root data dir (e.g. 'data/').\n",
    "    batch_time : str or datetime\n",
    "        Batch time for identification purposes. If str, in\n",
    "        format '%Y-%m-%dT%H:%M:%S'.\n",
    "    user_id : int\n",
    "        ID of the user mentioned.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    file_path : str\n",
    "        Filename, including path, where to save captured \n",
    "        mentions.\n",
    "    \"\"\"\n",
    "    return '{0:}{1:}/mentions_{1:}_{2:}.csv'.format(data_dir, parse_utc_time(batch_time, bsb2utc=False).strftime('%Y-%m-%dT%H:%M:%S'), user_id)\n",
    "\n",
    "def make_necessary_dirs(filename):\n",
    "    \"\"\"\n",
    "    Create directories in the path to `filename` (str), if necessary.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(filename)):\n",
    "        try:\n",
    "            os.makedirs(os.path.dirname(filename))\n",
    "        except OSError as exc: # Guard against race condition\n",
    "            if exc.errno != errno.EEXIST:\n",
    "                raise\n",
    "\n",
    "def run_batch_capture(batch_time, twitter_df, config, previous_batch=None, verbose=True, no_protected=False):\n",
    "    \"\"\"\n",
    "    Randomly select candidates and capture twitter mentions\n",
    "    to them.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_time : str or datetime\n",
    "        Approximately the current time, to serve only as \n",
    "        an identifier of the batch.\n",
    "    twitter_df : DataFrame\n",
    "        Table of all twitter IDs, to sample from.\n",
    "    config : dict\n",
    "        Capture process configuration, including capture\n",
    "        time window, folders for saving data and logs, \n",
    "        request rates.\n",
    "    previous_batch : DataFrame or None\n",
    "        If provided, do not sample IDs present in \n",
    "        `previous_batch`, to avoid data overlap.\n",
    "    verbose : bool\n",
    "        Whether to print capture counts ans status.\n",
    "    no_protected : bool\n",
    "        Whether to remove protected accounts from sampling.\n",
    "        I think protected accounts do not return replies, \n",
    "        but they still return mentions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    batch_df : DataFrame\n",
    "        List of sampled IDs, along with information about\n",
    "        their capture.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Parse datetime to str:\n",
    "    if type(batch_time) is dt.datetime:\n",
    "        batch_time = batch_time.strftime('%Y-%m-%dT%H:%M:%S')\n",
    "    \n",
    "    # Filter out protected twitter accounts is requested:\n",
    "    if no_protected is True:\n",
    "        ids_df = twitter_df.loc[twitter_df['protected'] == False]\n",
    "    else:\n",
    "        ids_df = twitter_df\n",
    "        \n",
    "    # Create batch of IDs to capture:\n",
    "    n_cands  = batch_n_cands(config['curr_level'], config['cap_renew_date'], config['avg_tweets_per_cand'], config['capture_period'])\n",
    "    batch_df = program_batch_capture(ids_df, n_cands, previous_batch, start_time=batch_time)\n",
    "\n",
    "    # Log batch data:\n",
    "    batch_df.to_csv('{}capture_{}.csv'.format(config['log_dir'], batch_time), index=False)\n",
    "\n",
    "    if verbose is True:\n",
    "        print('  ')\n",
    "    \n",
    "    # Loop over IDs to capture:\n",
    "    for i in batch_df.index.values:\n",
    "\n",
    "        try:\n",
    "            # Capture data:\n",
    "            mentions_df, stats = get_last_mentions(batch_df.loc[i, 'id'], config['capture_period'], verbose=False)\n",
    "            # Log capture statistics:\n",
    "            for name, stat in stats.items():\n",
    "                batch_df.loc[i, name] = stat\n",
    "            # Save captured mentions:\n",
    "            if mentions_df is not None:\n",
    "                filename = gen_mentions_path(config['data_dir'], batch_time, batch_df.loc[i, 'id'])\n",
    "                make_necessary_dirs(filename)\n",
    "                mentions_df.to_csv(filename, index=False)      \n",
    "            status = 'ok'\n",
    "        except:\n",
    "            # Record error:\n",
    "            tb = traceback.format_exc()\n",
    "            print_to_file(tb, '{}{}_i-{:05d}_id-{}.log'.format(config['error_dir'], batch_time, i, batch_df.loc[i, 'id']))\n",
    "            status = 'error'\n",
    "        \n",
    "        finally: \n",
    "            # Log batch data:\n",
    "            batch_df.loc[i, 'status'] = status\n",
    "            batch_df.to_csv('{}capture_{}.csv'.format(config['log_dir'], batch_time), index=False)\n",
    "            if verbose is True:\n",
    "                print('{}: {}'.format(i, status), end=', ')\n",
    "    \n",
    "    print('')\n",
    "    return batch_df\n",
    "\n",
    "def sum_batch_tweets(batch_df):\n",
    "    tot_tweets = int(batch_df.loc[batch_df['status'] == 'ok', 'batch_tweets'].sum())\n",
    "    return tot_tweets\n",
    "\n",
    "def compute_avg_tweets(batch_df):\n",
    "    \"\"\"\n",
    "    Compute the average number of tweets from \n",
    "    the capture batch and save it to the \n",
    "    config file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute average:\n",
    "    avg_tweets = batch_df.loc[batch_df['status'] == 'ok', 'batch_tweets'].mean()\n",
    "    avg_tweets = np.round(avg_tweets, 3)\n",
    "    \n",
    "    return avg_tweets\n",
    "    \n",
    "    # Update config:\n",
    "    config['avg_tweets_per_cand'] = avg_tweets\n",
    "    # Save config:\n",
    "    write_config(config)\n",
    "\n",
    "def next_batch_time(capture_period, ini_date='2022-08-12T00:00:00', date_fmt='%Y-%m-%dT%H:%M:%S'):\n",
    "    \"\"\"\n",
    "    Compute the datetime of the next batch from now.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    capture_period : float\n",
    "        Number of hours between each batch.\n",
    "    ini_date : str or datetime\n",
    "        Initial date (if str, in format `date_fmt`), from which\n",
    "        the following batches are scheduled.\n",
    "    date_fmt : str\n",
    "        Format of `ini_date`.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    next_date : datetime\n",
    "        When to run the next batch capture\n",
    "    \"\"\"\n",
    "    next_date = parse_utc_time(ini_date, bsb2utc=False)\n",
    "    now_date  = dt.datetime.now()\n",
    "    while next_date < now_date:\n",
    "        next_date = next_date + dt.timedelta(hours=capture_period)\n",
    "        \n",
    "    return next_date\n",
    "\n",
    "def load_saved_mentions(data_dir):\n",
    "    result_df = pd.concat([pd.read_csv(f, dtype={'in_reply_to_user_id':str}) for f in Path(data_dir).rglob('*.csv')], ignore_index=True)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38b89952-f59f-4741-85e9-bc68eeb88702",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_print(string, start=False):\n",
    "    print('{} {}: {}'.format('*' if start else ' ', dt.datetime.now().strftime('%Y-%m-%d %H:%M:%S'), string))\n",
    "\n",
    "def update_current_level(new_tweets, config):\n",
    "    \n",
    "    config['curr_level'] += new_tweets\n",
    "    write_config(config)\n",
    "\n",
    "def update_cap_renew_date(config, ini_date):\n",
    "\n",
    "    # Parse cap renew date:\n",
    "    utc_renew_time = parse_utc_time(config['cap_renew_date'] + 'T00:00:00', bsb2utc=False)\n",
    "    # Get time of the next batch:\n",
    "    next_time = next_batch_time(config['capture_period'], ini_date=ini_date)\n",
    "    \n",
    "    # Update cap renew date to next month if necessary:\n",
    "    if next_time >= utc_renew_time:\n",
    "        new_renew_time = utc_renew_time + dt.timedelta(days=30)\n",
    "        new_renew_time = new_renew_time + dt.timedelta(days=utc_renew_time.day - new_renew_time.day)\n",
    "    # Kepp current cap renew date:\n",
    "    else:\n",
    "        new_renew_time = utc_renew_time\n",
    "    \n",
    "    # Save new cap renew date:\n",
    "    config['cap_renew_date'] = new_renew_time.strftime('%Y-%m-%d')\n",
    "    write_config(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6ac7f6-9202-42a9-9265-771d255411a9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501158d4-723b-43aa-9717-f412e114e381",
   "metadata": {},
   "source": [
    "### Explorando a captura refeita no tutorial live-monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "577069a0-9973-4bca-8600-ed49a547adac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47e6f580-365a-42db-88d4-91d7b7409ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "b1_df = pd.read_csv('../tutoriais/live-monitor/tweets/logs/capture/capture_2023-02-27T12:30:00.csv')\n",
    "b2_df = pd.read_csv('../tutoriais/live-monitor/tweets/logs/capture/capture_2023-02-27T13:00:00.csv')\n",
    "b3_df = pd.read_csv('../tutoriais/live-monitor/tweets/logs/capture/capture_2023-02-27T13:30:00.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8c3b08-03a5-41db-af56-fe0d63c37440",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Arquivos CSV capturados e mal formados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79b78d8a-770a-4eed-9ae4-d9946a386155",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = pd.read_csv('../tweets/logs/capture/capture_2022-09-14T18:30:00.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc91d547-3f24-47f7-a4f7-51ea22ef5bd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cand_id_pos</th>\n",
       "      <th>id</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>batch_time</th>\n",
       "      <th>status</th>\n",
       "      <th>batch_start</th>\n",
       "      <th>batch_end</th>\n",
       "      <th>batch_tweets</th>\n",
       "      <th>batch_errors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>55</td>\n",
       "      <td>243326392</td>\n",
       "      <td>1158</td>\n",
       "      <td>2022-09-14 18:30:00</td>\n",
       "      <td>error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>1553</td>\n",
       "      <td>1541768269668966403</td>\n",
       "      <td>1158</td>\n",
       "      <td>2022-09-14 18:30:00</td>\n",
       "      <td>error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>934</th>\n",
       "      <td>2243</td>\n",
       "      <td>234888981</td>\n",
       "      <td>1158</td>\n",
       "      <td>2022-09-14 18:30:00</td>\n",
       "      <td>error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>951</th>\n",
       "      <td>1548</td>\n",
       "      <td>1557763774492409857</td>\n",
       "      <td>1158</td>\n",
       "      <td>2022-09-14 18:30:00</td>\n",
       "      <td>error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>632</td>\n",
       "      <td>1551612003097186305</td>\n",
       "      <td>1158</td>\n",
       "      <td>2022-09-14 18:30:00</td>\n",
       "      <td>error</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     cand_id_pos                   id  batch_size           batch_time status  \\\n",
       "600           55            243326392        1158  2022-09-14 18:30:00  error   \n",
       "893         1553  1541768269668966403        1158  2022-09-14 18:30:00  error   \n",
       "934         2243            234888981        1158  2022-09-14 18:30:00  error   \n",
       "951         1548  1557763774492409857        1158  2022-09-14 18:30:00  error   \n",
       "997          632  1551612003097186305        1158  2022-09-14 18:30:00  error   \n",
       "\n",
       "    batch_start batch_end  batch_tweets  batch_errors  \n",
       "600         NaN       NaN           NaN           NaN  \n",
       "893         NaN       NaN           NaN           NaN  \n",
       "934         NaN       NaN           NaN           NaN  \n",
       "951         NaN       NaN           NaN           NaN  \n",
       "997         NaN       NaN           NaN           NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_df.query('status == \"error\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67814393-246d-4145-ac10-ca125a4dc59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "carriage_files = ['../tweets/data/2022-08-18T09:30:00/mentions_2022-08-18T09:30:00_762402774260875265.csv',\n",
    "'../tweets/data/2022-08-18T09:30:00/mentions_2022-08-18T09:30:00_69373037.csv',\n",
    "'../tweets/data/2022-08-28T12:30:00/mentions_2022-08-28T12:30:00_975127727501185025.csv',\n",
    "'../tweets/data/2022-08-19T00:30:00/mentions_2022-08-19T00:30:00_74756085.csv',\n",
    "'../tweets/data/2022-09-01T06:30:00/mentions_2022-09-01T06:30:00_1198004545.csv',\n",
    "'../tweets/data/2022-09-14T18:30:00/mentions_2022-09-14T18:30:00_52045368.csv',\n",
    "'../tweets/data/2022-09-18T12:30:00/mentions_2022-09-18T12:30:00_31139434.csv',\n",
    "'../tweets/data/2022-09-18T12:30:00/mentions_2022-09-18T12:30:00_2319196454.csv',\n",
    "'../tweets/data/2022-08-29T09:30:00/mentions_2022-08-29T09:30:00_3096479489.csv']\n",
    "carriage_files = [s.replace('/data/', '/encrypted/') for s in carriage_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5702f00-444a-4055-89cc-f3a724c2a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** ../tweets/encrypted/2022-08-18T09:30:00/mentions_2022-08-18T09:30:00_762402774260875265.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna    N     %\n",
      "5  in_reply_to_user_id  9.0  1.13\n",
      "int64\n",
      "** ../tweets/encrypted/2022-08-18T09:30:00/mentions_2022-08-18T09:30:00_69373037.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna    N     %\n",
      "2  in_reply_to_user_id  1.0  1.56\n",
      "int64\n",
      "** ../tweets/encrypted/2022-08-28T12:30:00/mentions_2022-08-28T12:30:00_975127727501185025.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna     N     %\n",
      "1  in_reply_to_user_id  39.0  8.71\n",
      "int64\n",
      "** ../tweets/encrypted/2022-08-19T00:30:00/mentions_2022-08-19T00:30:00_74756085.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna     N     %\n",
      "3  in_reply_to_user_id  23.0  3.06\n",
      "int64\n",
      "** ../tweets/encrypted/2022-09-01T06:30:00/mentions_2022-09-01T06:30:00_1198004545.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna    N     %\n",
      "1  in_reply_to_user_id  9.0  2.34\n",
      "int64\n",
      "** ../tweets/encrypted/2022-09-14T18:30:00/mentions_2022-09-14T18:30:00_52045368.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "Empty DataFrame\n",
      "Columns: [coluna, N, %]\n",
      "Index: []\n",
      "int64\n",
      "** ../tweets/encrypted/2022-09-18T12:30:00/mentions_2022-09-18T12:30:00_31139434.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna     N     %\n",
      "4  in_reply_to_user_id  11.0  1.32\n",
      "int64\n",
      "** ../tweets/encrypted/2022-09-18T12:30:00/mentions_2022-09-18T12:30:00_2319196454.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "Empty DataFrame\n",
      "Columns: [coluna, N, %]\n",
      "Index: []\n",
      "int64\n",
      "** ../tweets/encrypted/2022-08-29T09:30:00/mentions_2022-08-29T09:30:00_3096479489.csv\n",
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna    N     %\n",
      "4  in_reply_to_user_id  6.0  3.02\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "for input_file in carriage_files:\n",
    "    print('**', input_file)\n",
    "    try:\n",
    "        df = pd.read_csv(input_file)\n",
    "    except pd.errors.ParserError:\n",
    "        print('use line terminator')\n",
    "        df = pd.read_csv(input_file, lineterminator='\\n')\n",
    "\n",
    "    xe.checkMissing(df)\n",
    "    print(df['batch_user'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a978a001-eb6c-4fa0-aeb7-dff18afd262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mColunas com valores faltantes:\u001b[0m\n",
      "                coluna    N     %\n",
      "4  in_reply_to_user_id  6.0  3.02\n"
     ]
    }
   ],
   "source": [
    "xe.checkMissing(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d1f65-c768-4121-a217-92733ace9f70",
   "metadata": {},
   "source": [
    "### Capturas que acusaram erro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd03935-9a1f-4323-b158-30394a5c3603",
   "metadata": {},
   "source": [
    "**ATTENTION:** The code in this section depends on a private key to run, which is not provided with the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccc91045-2d8d-460d-80a3-bdd9f974a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.xavy.twitter as xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6e2a67d-ac3d-4bc0-96c0-352fcf0fb365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dá erro porque o usuário não é encontrado:\n",
    "#url = get_mentions_in_period(1541768269668966403, '2022-09-15T09:30:00', '2022-09-15T12:30:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9207cf03-21fe-47bf-aca8-c883a0501e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificando um erro (com um usuário num certo intervalo de tempo):\n",
    "\n",
    "url = 'https://api.twitter.com/2/users/1530683046680305665/mentions'\n",
    "\n",
    "params = {'tweet.fields': ['created_at'],\n",
    " 'expansions': ['author_id',\n",
    "  'in_reply_to_user_id',\n",
    "  'entities.mentions.username'],\n",
    " 'max_results': 100,\n",
    " 'start_time': '2022-09-15T12:30:00Z',\n",
    " 'end_time': '2022-09-15T15:30:00Z'}\n",
    "\n",
    "mentions = xt.request_twitter_api(url, params, credentials='../../../config/keys/twitter_violentometro.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68242cbc-6ee1-4400-ac62-81df058e9419",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'meta': {'result_count': 0}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60a3945-c332-4e1f-a313-917684eb9931",
   "metadata": {},
   "source": [
    "### Cálculo de tamanho de batches (# de candidatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11004809-eda4-45f0-aa68-2ab318b3cf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1158.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = pd.read_csv('../dados/processados/twitter+insta+lagom_ids_deputados_2022.csv')\n",
    "len(twitter_df) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a609837d-9b4c-4623-8363-2777abb47404",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = read_config('../tweets/tweet_capture_config.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b38edeb-78f2-4d7b-a25f-95febe973da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_n_cands(8075, '2022-10-03', 4, 3, config['tweet_cap'], config['tweets_buffer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a140945-897b-4258-9c69-7988f69a884d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Captura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6bab35b1-a327-4ea1-905b-7a7e62d458df",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_df = pd.read_csv(Path('../tweets/logs/capture/capture_2022-08-16T18:30:00.csv'))\n",
    "\n",
    "def fake_run_batch_capture(batch_time, twitter_df, config, previous_batch=None, verbose=False, no_protected=False):\n",
    "    time.sleep(10)\n",
    "    return global_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4704c25f-c45e-4ed7-a817-c5e1a73c8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_time = '2022-08-14T09:14:00'\n",
    "batch_df = None\n",
    "config = read_config()\n",
    "while False:\n",
    "    \n",
    "    # Wait for next batch:\n",
    "    batch_time = next_batch_time(config['capture_period'], ini_date=batch_time)\n",
    "    log_print('Next batch at [{}]. Sleeping...'.format(batch_time))\n",
    "    sleep_time = (batch_time - dt.datetime.now()).total_seconds()\n",
    "    time.sleep(sleep_time)\n",
    "    \n",
    "    # Load data and config:\n",
    "    log_print('Reload config and ID pool!', True)\n",
    "    config = read_config()\n",
    "    twitter_df = pd.read_csv(config['twitter_ids_file'])\n",
    "    n_cands = batch_n_cands(config['curr_level'], config['cap_renew_date'], config['avg_tweets_per_cand'], config['capture_period'], config['tweet_cap'], config['tweets_buffer'])\n",
    "    config_message = 'Batch config! # cands: {:d}, current level: {:d}, cap renew date: {}, avg. tweets p. cand: {:.3f}, capture period: {:.3f}'\n",
    "    log_print(config_message.format(n_cands, config['curr_level'], config['cap_renew_date'], config['avg_tweets_per_cand'], config['capture_period']))\n",
    "    \n",
    "    # Run next batch:\n",
    "    log_print('Running batch...')\n",
    "    batch_df = run_batch_capture(batch_time, twitter_df, config, batch_df)\n",
    "    tot_tweets = sum_batch_tweets(batch_df)\n",
    "    avg_tweets = compute_avg_tweets(batch_df)\n",
    "    log_print('Finished batch! Tweets captured: {:d}, Avg tweets: {:.3f}'.format(tot_tweets, avg_tweets))\n",
    "    \n",
    "    # Update config:\n",
    "    update_current_level(tot_tweets, config)\n",
    "    update_cap_renew_date(config, batch_time)\n",
    "    log_print('Updated config! current level: {:d}, cap renew date: {}'.format(config['curr_level'], config['cap_renew_date']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d98ccc-01d1-411e-8a46-a841a9b39306",
   "metadata": {},
   "source": [
    "## Testes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b3426-3b0b-4bc1-aa15-e45a8e1b108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carrega os dados capturados:\n",
    "result_df = load_saved_mentions(config['data_dir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "2c17c79e-0924-40a7-92ae-ed59c1289c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from neuralmind/bert-base-portuguese-cased\n",
      "Loading trained model: ../modelos/bertimbau-hatespeech-v01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-13 11:08:47.970692: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_UNKNOWN: unknown error\n",
      "2022-08-13 11:08:47.970737: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: glitterbu\n",
      "2022-08-13 11:08:47.970746: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: glitterbu\n",
      "2022-08-13 11:08:47.970881: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 390.154.0\n",
      "2022-08-13 11:08:47.970909: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 390.154.0\n",
      "2022-08-13 11:08:47.970917: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 390.154.0\n",
      "2022-08-13 11:08:47.991092: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some layers from the model checkpoint at ../modelos/bertimbau-hatespeech-v01 were not used when initializing TFBertForSequenceClassification: ['dropout_151']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ../modelos/bertimbau-hatespeech-v01.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Adiciona nota do modelo aos dados:\n",
    "model  = sw.HateSpeechModel('../modelos/bertimbau-hatespeech-v01')\n",
    "y_pred = model.predict_proba(result_df['text'])\n",
    "result_df['hate_score'] = pd.Series(y_pred, index=result_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40edc47d-adaa-41e6-a979-c642a0cdd569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print captured mentions of a target user:\n",
    "username = 'carteiroreaca'\n",
    "target_id = twitter_df.query('username == \"{}\"'.format(username))['id'].iloc[0]\n",
    "#xd.print_string_series(result_df.loc[target_id == result_df['batch_user'], 'text']) # Output removed to avoid tweet links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6dd56f-2934-40c4-9a64-4541646e02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print batch pages:\n",
    "check_df = batch_df.join('https://twitter.com/' + twitter_df.set_index('id')['username'], on = 'id')\n",
    "#xd.print_string_series(check_df.set_index('batch_tweets')['username'].sample(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c4ec28-9225-4b79-aa06-cd849201855b",
   "metadata": {},
   "source": [
    "## Testando a API v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f514088a-5059-48d1-bd0b-a3f4735c80bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://api.twitter.com/2/users/1001251931812220928/timelines/reverse_chronological')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5ee8f-127f-43f3-99a3-a503bbf4ac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = tw.request_twitter_api('https://api.twitter.com/2/users/1001251931812220928/timelines/reverse_chronological', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "428e3bb5-7185-4b90-8093-7f6ae65f163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.twitter.com/2/users/1001251931812220928/mentions'\n",
    "params = {'max_results': 10, 'expansions':['author_id','in_reply_to_user_id']}\n",
    "mentions = tw.request_twitter_api(url, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97226a-bbf1-4163-8438-6c51243c6076",
   "metadata": {},
   "outputs": [],
   "source": [
    "mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f260a0c-eb21-4583-8627-c973e79cbd8e",
   "metadata": {},
   "source": [
    "## Testando a API v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead2eb61-a747-4829-820e-f7acda3096a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = tw.get_timeline('tabataamaralsp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5292d120-6f86-41ae-bf98-7276d14abba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "replies = tw.tweets_lookup(reply_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb7d34c-9767-40bc-936a-160f23ee9884",
   "metadata": {},
   "source": [
    "## Lixo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826c67fe-2ae3-40a1-91af-f1321a27bb73",
   "metadata": {},
   "source": [
    "### Carregando dados de redes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69aa2227-18c3-4a76-bb08-adc8e37a4d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TSE data and extract twitter usernames:\n",
    "redes20_df = pd.read_csv('../dados/processados/redes_candidatos_2022.csv')\n",
    "twitter_df = redes20_df.query('twitter == 1')[['SQ_CANDIDATO', 'DS_URL']].copy()\n",
    "twitter_df['twitter_username'] = extract_twitter_username(twitter_df['DS_URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40cc9364-c7cb-4315-a4b1-822f68572293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Down-sample to test functions:\n",
    "test_twitter_df = twitter_df.sample(20, random_state=56236101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bde5cf75-2a15-4bb8-bef6-b4e9bf5002f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_df = request_twitter_user_info(test_twitter_df)\n",
    "# Save results:\n",
    "#response_df.to_json('../dados/brutos/twitter/cand_twitter_user_info_TESTE.json', force_ascii=False, orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f569f9ea-7a91-4e86-b530-8df0652cf687",
   "metadata": {},
   "source": [
    "### Teste de concatenação de JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f277d0e9-44dd-473a-8425-c92fd353af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xavy.test as xtest\n",
    "\n",
    "cin = [(1,2), ([1,2], [3,4]), ('a', 'b'), (['a', 'b'], ['c', 'd']), ({'a': 1}, {'a': 2}), ({'a': 1}, {'b': 2}), ({'a': 1, 'b':0}, {'b': 2}),\n",
    "       ({'a': [1, 2], 'b':[0,1]}, {'b': 2}), ([{'a':1}, {'b':2}], [{'c':3}, {'d':4}]), ({'a':{'A': 1, 'B': 2}}, {'a':{'A': 2, 'B': 3}}),\n",
    "       ({'a':{'A': 1, 'B': [0,1,2]}, 'b':[1,2,3]}, {'a':{'A': [2,3,4], 'B': 3}})]\n",
    "cout = [[1,2], [1,2,3,4], ['a', 'b'], ['a', 'b', 'c', 'd'], {'a':[1,2]}, {'a':1, 'b': 2}, {'a':1, 'b': [0,2]}, \n",
    "        {'a':[1, 2], 'b': [0,1,2]}, [{'a':1}, {'b':2}, {'c':3}, {'d':4}], {'a':{'A':[1,2], 'B':[2,3]}}, \n",
    "        {'a':{'A':[1,2,3,4], 'B':[0,1,2,3]}, 'b':[1,2,3]}]\n",
    "xtest.multi_test_function(tw.concat_json_pages, cin, cout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df364e5-0089-435a-b749-d0be1c63c56f",
   "metadata": {},
   "source": [
    "### Funções velhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a05e7a92-d1ca-4c02-b138-b6e1aff5e3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_mentions(mentions1, mentions2):\n",
    "    \"\"\"\n",
    "    Concatenate twitter API responses from\n",
    "    /2/users/:id/mentions endpoint maintaining\n",
    "    the same data structure of lists inside \n",
    "    mentions dict keys. mentions keys that are\n",
    "    not lists are turned into lists and \n",
    "    concatenated.\n",
    "    \n",
    "    Returns a dict.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if API response contains unknown fields:\n",
    "    known_keys = ['data', 'includes', 'errors', 'meta']\n",
    "    unknown_keys = set(mentions1.keys()) - set(known_keys)\n",
    "    assert len(unknown_keys) == 0, 'Found unknown keys {} in mentions1'.format(unknown_keys)\n",
    "    unknown_keys = set(mentions2.keys()) - set(known_keys)\n",
    "    assert len(unknown_keys) == 0, 'Found unknown keys {} in mentions2'.format(unknown_keys)\n",
    "    \n",
    "    # Create input \"with all keys\":\n",
    "    m1 = defaultdict(lambda: [], mentions1)\n",
    "    m2 = defaultdict(lambda: [], mentions2)\n",
    "    \n",
    "    mentions3 = dict()\n",
    "    # Concatenate data:\n",
    "    for k in known_keys:\n",
    "        \n",
    "        # Standardize data as lists:\n",
    "        if type(m1[k]) != list:\n",
    "            m1[k] = [m1[k]]\n",
    "        if type(m2[k]) != list:\n",
    "            m2[k] = [m2[k]]\n",
    "        \n",
    "        # Concatenate lists:\n",
    "        mentions3[k] = m1[k] + m2[k]\n",
    "    \n",
    "    return mentions3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "60dfce40-bd33-464d-9609-0c27f77c2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def program_day_capture(twitter_df, n_cands, capture_period, date=None, hour_offset='00:10:00', random_state=None):\n",
    "    \"\"\"\n",
    "    Generate DataFrame with a schedule for capturing data from \n",
    "    randomly sampled candidates, organized in batches spaced \n",
    "    during the day.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    twitter_df : DataFrame\n",
    "        Table with column 'id' containing all the candidates'\n",
    "        Twitter IDs.\n",
    "    n_cands : int\n",
    "        Number of candidates to randomly select for today's \n",
    "        capture.\n",
    "    date : str or None\n",
    "        Date to schedule the capture on, in format '%Y-%m-%d'.\n",
    "        If None, get the current date.\n",
    "    hour_offset : str\n",
    "        First time during this day that the capture will run,\n",
    "        in format '%H:%M:%S'.\n",
    "    random_state : int or None\n",
    "        Seed for randomly selecting candidates. Use None for \n",
    "        random seed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : DataFrame\n",
    "        Table with candidates to capture, their batches and \n",
    "        the time they should be captured.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get current date if needed:\n",
    "    if date is None:\n",
    "        date = dt.date.today().strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Set first datetime of today's capture:\n",
    "    dt_offset = parse_utc_time(date + 'T' + hour_offset, bsb2utc=False)\n",
    "    \n",
    "    # Randomly select candidates:\n",
    "    daily_capture_df = twitter_df['id'].sample(n_cands, random_state=random_state).reset_index()\n",
    "    daily_capture_df.rename({'index':'cand_id_pos', 'id':'user_id'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Prepare batch information:\n",
    "    daily_capture_df['batch']   = (np.arange(n_cands) / int(n_cands / (24 / capture_period) + 1)).astype(int)\n",
    "    batch_size = daily_capture_df['batch'].value_counts()\n",
    "    batch_size.name  = 'batch_size'\n",
    "    daily_capture_df = daily_capture_df.join(batch_size, on='batch')\n",
    "    daily_capture_df['batch_time'] = dt_offset + daily_capture_df['batch'] * dt.timedelta(hours=capture_period)\n",
    "    daily_capture_df['status'] = 'queue'\n",
    "    \n",
    "    return daily_capture_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
