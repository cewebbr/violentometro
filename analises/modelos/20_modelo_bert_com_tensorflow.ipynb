{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxEWIBJ7Uv3g",
    "tags": []
   },
   "source": [
    "# Criando modelo p/ discurso de ódio c/ BERT pré-treinado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cDbCEufIUv3i"
   },
   "source": [
    "[Voltar ao Índice](../00_indice.ipynb)\n",
    "\n",
    "Vamos usar o modelo BERT pré-treinado (multilingual, p/ funcionar com o português) para classificar discursos de ódio. \n",
    "Na verdade, buscaremos reproduzir o código do Diogo Cortiz (que usa o pytorch), que reproduzimos [aqui](https://colab.research.google.com/drive/18YXlk-ZIlAymoOYn5nJQE16I3SsguUwq)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJdq4PxFUz8t",
    "outputId": "530e75e4-58f9-4bad-c94e-8ca1d51ee696"
   },
   "outputs": [],
   "source": [
    "# P/ rodar no Colab:\n",
    "#!pip install transformers\n",
    "#!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "AhaiLEsUUv3k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 09:12:50.736590: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "# Hugging Face:\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from datasets import Dataset\n",
    "from transformers import DefaultDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N-zKEEEZaw0r"
   },
   "outputs": [],
   "source": [
    "# P/ rodar no Colab:\n",
    "# Para baixar um arquivo do Google Drive:\n",
    "#!pip install -U -q PyDrive\n",
    "\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials# Authenticate and create the PyDrive client.\n",
    "\n",
    "##auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KyBulMSvUv3n"
   },
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "GeWCwDtKUv3o"
   },
   "outputs": [],
   "source": [
    "###########################################\n",
    "### Splitting datasets into random sets ###\n",
    "###########################################\n",
    "\n",
    "def shuffled_pos(length, seed):\n",
    "    \"\"\"\n",
    "    Return indices from 0 to `length` - 1 in a shuffled state, given random `seed`.\n",
    "    \"\"\"\n",
    "    return np.random.RandomState(seed=seed).permutation(length)\n",
    "\n",
    "\n",
    "def random_index_sets(size, set_fracs, seed):\n",
    "    \"\"\"\n",
    "    Return sets of random indices (from 0 to `size` - 1) with lengths \n",
    "    given by ~ `size` * `set_fracs`.\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    size : int\n",
    "        The size of the index list to split into sets.\n",
    "        \n",
    "    set_fracs : iterable\n",
    "        The fractions of the list of indices that each index set \n",
    "        should contain. \n",
    "    \n",
    "    seed : int\n",
    "        The seed for the random number generator.\n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    indices : tuple of arrays\n",
    "        The indices for each set.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert np.isclose(np.sum(set_fracs), 1), '`set_fracs` should add up to one.'\n",
    "    \n",
    "    # Create randomized list of indices:\n",
    "    shuffled_indices = shuffled_pos(size, seed)\n",
    "    \n",
    "    \n",
    "    indices   = []\n",
    "    set_start = [0]\n",
    "    # Determine the sizes of the sets:\n",
    "    set_sizes = [round(size * f) for f in set_fracs]\n",
    "    set_sizes[0] = size - sum(set_sizes[1:])\n",
    "    assert np.sum(set_sizes) == size, 'Set sizes should add up to total size.'\n",
    "    \n",
    "    for i in range(0, len(set_fracs) - 1):\n",
    "        # Select indices for a set:\n",
    "        set_start.append(set_start[i] + set_sizes[i])\n",
    "        set_indices = shuffled_indices[set_start[i]:set_start[i + 1]]\n",
    "        indices.append(set_indices)\n",
    "        assert len(indices[i]) == len(set(indices[i])), 'There are repeating indices in a set.'\n",
    "        \n",
    "    # Select the indices for the last set:\n",
    "    indices.append(shuffled_indices[set_start[-1]:])\n",
    "    assert len(set(np.concatenate(indices))) == sum([len(i) for i in indices]), \\\n",
    "    'There are common indices between sets.'\n",
    "    \n",
    "    return tuple(indices)\n",
    "\n",
    "\n",
    "def random_set_split(df, set_fracs, seed):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into randomly selected disjoint and complete sets.\n",
    "    \n",
    "    \n",
    "    Input\n",
    "    -----\n",
    "    \n",
    "    df : Pandas DataFrame\n",
    "        The dataframe to split into a complete and disjoint set of sub-sets.\n",
    "        \n",
    "    set_fracs : array-like\n",
    "        The fraction of `df` that should be put into each set. The length of \n",
    "        `set_fracs` determines the number of sub-sets to create.\n",
    "    \n",
    "    seed : int\n",
    "        The seed for the random number generator used to split `df`.\n",
    "        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "    A tuple of DataFrames, one for each fraction in `set_fracs`, in that order.\n",
    "    \"\"\"\n",
    "    # Get positional indices for each set:\n",
    "    sets_idx = random_index_sets(len(df), set_fracs, seed)\n",
    "    \n",
    "    return tuple(df.iloc[idx] for idx in sets_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S-Q8IyPiUv3q"
   },
   "outputs": [],
   "source": [
    "def process_pandas_to_tfdataset(df, tokenizer, max_length=80, shuffle=True, text_col='text', target_col='label', batch_size=8):\n",
    "    \"\"\"\n",
    "    Prepare NLP data in a Pandas DataFrame to be used \n",
    "    in a TensorFlow transformer model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        The corpus, containing the columns `text_col` \n",
    "        (the sentences) and `target_col` (the labels).\n",
    "    tokenizer : HuggingFace AutoTokenizer\n",
    "        A tokenizer loaded from \n",
    "        `transformers.AutoTokenizer.from_pretrained()`.\n",
    "    max_length : int\n",
    "        Maximum length of the sentences (smaller \n",
    "        sentences will be padded and longer ones\n",
    "        will be truncated). This is required for \n",
    "        training, so batches have instances of the\n",
    "        same shape.\n",
    "    shuffle : bool\n",
    "        Shuffle the dataset order when loading. \n",
    "        Recommended True for training, False for \n",
    "        validation/evaluation.\n",
    "    text_col : str\n",
    "        Name of `df` column containing the sentences.\n",
    "    target_col : str\n",
    "        Name of `df` column containing the labels of \n",
    "        the sentences.\n",
    "    batch_size : int\n",
    "        The size of the batch in the output \n",
    "        tensorflow dataset.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    tf_dataset : TF dataset\n",
    "        A dataset that can be fed into a transformer \n",
    "        model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Security checks:\n",
    "    renamed_df = df.rename({target_col:'labels'}, axis=1) # Hugging Face requer esse nome p/ y.\n",
    "    \n",
    "    # Define função para processar os dados com o tokenizador:\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[text_col], padding=True, max_length=max_length, truncation=True)\n",
    "    \n",
    "    # pandas -> hugging face:\n",
    "    hugging_set = Dataset.from_pandas(renamed_df)\n",
    "    # texto -> sequência de IDs: \n",
    "    encoded_set = hugging_set.map(tokenize_function, batched=True)\n",
    "    # hugging face -> tensorflow dataset:\n",
    "    data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "    tf_dataset = encoded_set.to_tf_dataset(columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"], label_cols=[\"labels\"], shuffle=shuffle, collate_fn=data_collator, batch_size=batch_size)\n",
    "    \n",
    "    return tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_aL_-u_oUv3q"
   },
   "outputs": [],
   "source": [
    "def gen_tensorboard_callback(root_dir, run_name):\n",
    "    \"\"\"\n",
    "    Return a tensorboard callback with log dir given \n",
    "    by `root_dir` + `run_name`. It avoids logging \n",
    "    to a pre-existing log inadvertently. \n",
    "    \"\"\"\n",
    "    \n",
    "    # Root dir should exist. Check it:\n",
    "    if os.path.isdir(root_dir) == False:\n",
    "        raise Exception(\"`root_dir` {} is unknown.\".format(root_dir))\n",
    "    \n",
    "    # Build path to log:\n",
    "    fullpath = os.path.join(root_dir, run_name)\n",
    "    \n",
    "    # Check if log already exists:\n",
    "    already_exists = os.path.isdir(fullpath)\n",
    "    if already_exists:\n",
    "        \n",
    "        # If exists, ask if it sohuld continue:\n",
    "        go_on = input(\"Run log '{}' already exists. Continue (y/n)?\".format(run_name))\n",
    "        if go_on == 'y' or go_on == 'Y':\n",
    "            return tf.keras.callbacks.TensorBoard(fullpath)\n",
    "       \n",
    "        else:\n",
    "            raise Exception('Abort so not to mess with tensorboard log.')\n",
    "    \n",
    "    else:\n",
    "        return tf.keras.callbacks.TensorBoard(fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wW_bPXvSJYFd"
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, tf_dataset):\n",
    "    \"\"\"\n",
    "    Use the provided model to compute the\n",
    "    probability that each instance is \n",
    "    in the positive class (1 in a binary \n",
    "    classification).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : TFBertForSequenceClassification\n",
    "        A Hugging Face implementation of a \n",
    "        Tensorflow transformer model.\n",
    "    tf_dataset : Tensorflow Dataset\n",
    "        The data for which to make predictions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    probs : array\n",
    "        Probability that the corresponding \n",
    "        instance falls in the positive class\n",
    "        (y = 1).\n",
    "    \"\"\"\n",
    "\n",
    "    tf_predict = model.predict(tf_dataset).logits\n",
    "    probs = tf.sigmoid(tf_predict)[:,0].numpy()\n",
    "    \n",
    "    return probs\n",
    "\n",
    "\n",
    "def predict_class(model, tf_dataset, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Use the provided model to predict\n",
    "    the class of each instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : TFBertForSequenceClassification\n",
    "        A Hugging Face implementation of a \n",
    "        Tensorflow transformer model.\n",
    "    tf_dataset : Tensorflow Dataset\n",
    "        The data for which to make predictions.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    preds : array\n",
    "        Predicted class for the corresponding\n",
    "        instances.\n",
    "    \"\"\"\n",
    "\n",
    "    probs = predict_proba(model, tf_dataset)\n",
    "    preds = (probs > threshold).astype(int)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvjnMPEOUv3r"
   },
   "source": [
    "## Carregando o BERTimbau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "buWofr5TUv3s",
    "outputId": "56f6ed11-d087-4042-8037-056c788d6d49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-18 09:12:56.847540: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:56.864711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:56.864873: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:56.865648: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-18 09:12:56.865965: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:56.866073: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:56.866150: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:57.335140: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:57.335345: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:57.335481: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-11-18 09:12:57.335787: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 968 MB memory:  -> device: 0, name: NVIDIA GeForce MX450, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier', 'bert/pooler/dense/bias:0', 'bert/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define o modelo em questão:\n",
    "model_name = 'neuralmind/bert-base-portuguese-cased'\n",
    "# Carregando:\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
    "model      = TFAutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUFcDa5OUv3u"
   },
   "source": [
    "## Carregando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EeEBAW4UUv3u"
   },
   "source": [
    "Fonte: Juntamos os dados de Fortuna e Pelle (veja o notebook do modelo baseline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VE9Xm5NybMra"
   },
   "outputs": [],
   "source": [
    "# P/ rodar no Colab:\n",
    "# Para baixar os dados do Google Drive p/ o Colab:\n",
    "#link = 'https://drive.google.com/file/d/1Xq_xPg-OA3q0pfIOf7oIvbc0xsOQ0sxa/view?usp=sharing'\n",
    "#link_id = link.split('/')[-2]\n",
    "#downloaded = drive.CreateFile({'id':link_id}) \n",
    "#downloaded.GetContentFile('hatespeech_fortuna3+offcombr2.csv')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "ZFLE75DaUv3u"
   },
   "outputs": [],
   "source": [
    "# Carrega os dados:\n",
    "mass_df = pd.read_csv('../../dados/processados/hatespeech_fortuna3+offcombr2.csv')\n",
    "#mass_df = pd.read_csv('hatespeech_fortuna3+offcombr2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    4713\n",
       "1     926\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Quantidade de dados em cada classe:\n",
    "mass_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "PhkEC1VGUv3v"
   },
   "outputs": [],
   "source": [
    "# Separa os dados em amostras:\n",
    "train_df, val_df, test_df = random_set_split(mass_df, [0.7, 0.15, 0.15], 1323)\n",
    "#train_df, val_df = random_set_split(mass_df, [0.85, 0.15], 45998)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando dados p/ teste do modelo:\n",
    "#val_df.to_csv('../../dados/processados/hatespeech_fortuna3+offcombr2_val_seed1323.csv', index=False)\n",
    "#test_df.to_csv('../../dados/processados/hatespeech_fortuna3+offcombr2_test_seed1323.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "6250c40f8e114db9a226db01ce2a1d84",
      "bae291373ce24ce1a0594334ae2ce414",
      "734e8736e4b948c790536a524aaf7f01",
      "9d91c870729c4cfba700acda20eeef0d",
      "4e62dbd2382d4d15aa49cff8de61be53",
      "cd5d6dfa298b4756a6d4e714cdefedd5",
      "b2ae2782be834feeaffde4e50fc0e3f2",
      "585fa61fe96b4c11bf0163b3ea646cd2",
      "976513a8ec674d3fb16cb04d8166533a",
      "bfe43e79dae34624bad82ab5cab0e4bf",
      "28f9d87645184fca8b8305167b4c2539",
      "dd64a6ae604847808188a63f60b9dd6e",
      "3f51f1a3231d4b4eb20b407b5664f78f",
      "d390ffb74d5044f8b0c847c30191957c",
      "11b3d04dc0f3446d8b7d007aaa923865",
      "7f0a57db827b4b47a492759ac218377d",
      "8b6ca3c2ac2c4f4c88c9528f52f075a3",
      "707bd0c951ff46d88df758e3dfc7faec",
      "28ac59e6f3ad4442837e470d63a5913a",
      "3fba13def095450f9c9b2dd0c65f5fd0",
      "204739ce239b47cb86643f556939e81f",
      "0a3754eebd83498eaf19c4dbd836d6fb"
     ]
    },
    "id": "ft2GXmXXUv3v",
    "outputId": "32889dbc-1eef-4451-cfbe-945a74c0f08c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4b1e901920466b85c8557c0cccf707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb325f64d91f484bb92ed85e6734df09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokeniza os textos e os coloca no formato do Tensorflow Dataset:\n",
    "train_tfd = process_pandas_to_tfdataset(train_df, tokenizer, batch_size=32, shuffle=True)\n",
    "val_tfd   = process_pandas_to_tfdataset(val_df, tokenizer, batch_size=32, shuffle=False)\n",
    "#test_tfd  = process_pandas_to_tfdataset(test_df, tokenizer, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0rB0mOLwWZoa",
    "outputId": "128b2c6e-461c-405e-959d-e6bf581cd18f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8262411347517731"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Acurácia mínima (chute a moda):\n",
    "(val_df['label'] == 0).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlfYNHIIUv3w"
   },
   "source": [
    "## Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6F9uJXeyKyEN"
   },
   "outputs": [],
   "source": [
    "# Parâmetros do treinamento:\n",
    "model_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True) # O Hugging Face não coloca uma função de ativação na última camada, por isso usaremos 'logits'.\n",
    "metrics = ['accuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nctJ4r_K5Jc"
   },
   "source": [
    "### Início do treinamento: ajuste grosso da última camada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hP1XMQFzTMym"
   },
   "source": [
    "Nesta etapa, não esperamos que haja overfitting pois o modelo é muito simples (basicamente uma regressão logística sobre as features criadas pelo BERT. Na verdade, devemos ter um underfitting. Podemos treinar à vontade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drPqt_vTUv3w",
    "outputId": "02914e17-e776-42c6-b86a-500730fa326b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108923136 \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,923,905\n",
      "Trainable params: 769\n",
      "Non-trainable params: 108,923,136\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preparando o modelo com o BERT congelado:\n",
    "optimizer  = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.get_layer('bert').trainable = False\n",
    "model.compile(optimizer, model_loss, metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "SNHgYNyPUv3x"
   },
   "outputs": [],
   "source": [
    "# Monitoramento com o Tensorboard \n",
    "# tensorboard --logdir=tensor_logs/\n",
    "#board = gen_tensorboard_callback('tensor_logs/', 'first_try')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9E2vMgRzsh1_",
    "outputId": "9de84bfb-c542-4c68-e863-cd81784c7d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      " 60/124 [=============>................] - ETA: 1:12 - loss: 0.4815 - accuracy: 0.8286"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Ajustando o modelo:\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_tfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_tfd\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2451\u001b[0m   (graph_function,\n\u001b[1;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1863\u001b[0m     args,\n\u001b[1;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1865\u001b[0m     executing_eagerly)\n\u001b[1;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Ajustando o modelo:\n",
    "model.fit(train_tfd, epochs=20, validation_data=val_tfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperamos chegar em algo perto de:\n",
    "\n",
    "    loss: 0.3111 - accuracy: 0.8610 - val_loss: 0.3074 - val_accuracy: 0.8617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J5P4vEJ3LK42"
   },
   "source": [
    "### Ajuste fino da última camada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OC8yda7iuBMH"
   },
   "outputs": [],
   "source": [
    "# Vamos baixar a taxa de aprendizado:\n",
    "optimizer  = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
    "model.compile(optimizer, model_loss, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E8o2ME3Gti01",
    "outputId": "b726e639-9fe3-4035-ccc6-e55127feeddf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "150/150 [==============================] - 47s 236ms/step - loss: 0.3062 - accuracy: 0.8621 - val_loss: 0.3074 - val_accuracy: 0.8617\n",
      "Epoch 2/10\n",
      "150/150 [==============================] - 33s 221ms/step - loss: 0.3093 - accuracy: 0.8588 - val_loss: 0.3074 - val_accuracy: 0.8617\n",
      "Epoch 3/10\n",
      "150/150 [==============================] - 33s 222ms/step - loss: 0.3139 - accuracy: 0.8573 - val_loss: 0.3073 - val_accuracy: 0.8605\n",
      "Epoch 4/10\n",
      "150/150 [==============================] - 33s 222ms/step - loss: 0.3032 - accuracy: 0.8631 - val_loss: 0.3073 - val_accuracy: 0.8617\n",
      "Epoch 5/10\n",
      "150/150 [==============================] - 33s 221ms/step - loss: 0.3132 - accuracy: 0.8615 - val_loss: 0.3073 - val_accuracy: 0.8617\n",
      "Epoch 6/10\n",
      "150/150 [==============================] - 33s 222ms/step - loss: 0.3085 - accuracy: 0.8625 - val_loss: 0.3072 - val_accuracy: 0.8617\n",
      "Epoch 7/10\n",
      "150/150 [==============================] - 33s 222ms/step - loss: 0.3107 - accuracy: 0.8594 - val_loss: 0.3072 - val_accuracy: 0.8605\n",
      "Epoch 8/10\n",
      "150/150 [==============================] - 35s 230ms/step - loss: 0.3092 - accuracy: 0.8631 - val_loss: 0.3071 - val_accuracy: 0.8617\n",
      "Epoch 9/10\n",
      "150/150 [==============================] - 35s 234ms/step - loss: 0.3110 - accuracy: 0.8617 - val_loss: 0.3071 - val_accuracy: 0.8605\n",
      "Epoch 10/10\n",
      "150/150 [==============================] - 33s 222ms/step - loss: 0.3077 - accuracy: 0.8621 - val_loss: 0.3071 - val_accuracy: 0.8593\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57f7ee2510>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustando o modelo:\n",
    "model.fit(train_tfd, epochs=10, validation_data=val_tfd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperamos chegar em algo perto de:\n",
    "\n",
    "    loss: 0.3077 - accuracy: 0.8621 - val_loss: 0.3071 - val_accuracy: 0.8593"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NFTaUoAjLRDq"
   },
   "source": [
    "### Liberar o modelo todo para treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NO5uvF_kLpmt"
   },
   "source": [
    "Agora é importante ir acompanhando a evolução da função de custo tanto para a amostra de treinamento quanto para a amostra de validação. \n",
    "\n",
    "* Uma boa taxa de aprendizado deve levar a uma queda gradual da função de custo na amostra de treinamento. Para não bagunçar os pesos, vamos baixar bastante a taxa de aprendizado.\n",
    "\n",
    "* Quando a função de custo parar de baixar para a amostra de validação, entramos no regime de overfitting. É preciso parar o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VM8by-dpfVYV",
    "outputId": "e97fc9b9-850d-44c7-ee6d-80e21fee713e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_sequence_classification_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bert (TFBertMainLayer)      multiple                  108923136 \n",
      "                                                                 \n",
      " dropout_75 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  769       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 108,923,905\n",
      "Trainable params: 108,923,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Preparando o modelo com o BERT livre p/ ajustes (vamos baixar ainda mais a taxa de aprendizado):\n",
    "optimizer  = tf.keras.optimizers.Adam(learning_rate=5e-7)\n",
    "model.get_layer('bert').trainable = True\n",
    "model.compile(optimizer, model_loss, metrics)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8_jnIqifjOD",
    "outputId": "c36acde9-1c87-4524-afcc-03885b74c97d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "150/150 [==============================] - 94s 550ms/step - loss: 0.3066 - accuracy: 0.8615 - val_loss: 0.3044 - val_accuracy: 0.8617\n",
      "Epoch 2/40\n",
      "150/150 [==============================] - 79s 529ms/step - loss: 0.3046 - accuracy: 0.8642 - val_loss: 0.3018 - val_accuracy: 0.8593\n",
      "Epoch 3/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2893 - accuracy: 0.8709 - val_loss: 0.2994 - val_accuracy: 0.8676\n",
      "Epoch 4/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2858 - accuracy: 0.8748 - val_loss: 0.2962 - val_accuracy: 0.8664\n",
      "Epoch 5/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2797 - accuracy: 0.8763 - val_loss: 0.2938 - val_accuracy: 0.8664\n",
      "Epoch 6/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2733 - accuracy: 0.8744 - val_loss: 0.2922 - val_accuracy: 0.8664\n",
      "Epoch 7/40\n",
      "150/150 [==============================] - 79s 528ms/step - loss: 0.2689 - accuracy: 0.8794 - val_loss: 0.2903 - val_accuracy: 0.8688\n",
      "Epoch 8/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2648 - accuracy: 0.8819 - val_loss: 0.2888 - val_accuracy: 0.8747\n",
      "Epoch 9/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2556 - accuracy: 0.8819 - val_loss: 0.2878 - val_accuracy: 0.8735\n",
      "Epoch 10/40\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.2573 - accuracy: 0.8838 - val_loss: 0.2863 - val_accuracy: 0.8747\n",
      "Epoch 11/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2534 - accuracy: 0.8871 - val_loss: 0.2859 - val_accuracy: 0.8759\n",
      "Epoch 12/40\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.2452 - accuracy: 0.8903 - val_loss: 0.2875 - val_accuracy: 0.8735\n",
      "Epoch 13/40\n",
      "150/150 [==============================] - 79s 525ms/step - loss: 0.2396 - accuracy: 0.8938 - val_loss: 0.2882 - val_accuracy: 0.8747\n",
      "Epoch 14/40\n",
      "150/150 [==============================] - 79s 527ms/step - loss: 0.2360 - accuracy: 0.8961 - val_loss: 0.2853 - val_accuracy: 0.8783\n",
      "Epoch 15/40\n",
      "150/150 [==============================] - 79s 528ms/step - loss: 0.2284 - accuracy: 0.8984 - val_loss: 0.2833 - val_accuracy: 0.8783\n",
      "Epoch 16/40\n",
      "150/150 [==============================] - 79s 525ms/step - loss: 0.2266 - accuracy: 0.8984 - val_loss: 0.2835 - val_accuracy: 0.8783\n",
      "Epoch 17/40\n",
      "150/150 [==============================] - 79s 525ms/step - loss: 0.2212 - accuracy: 0.9013 - val_loss: 0.2849 - val_accuracy: 0.8818\n",
      "Epoch 18/40\n",
      "150/150 [==============================] - 79s 525ms/step - loss: 0.2154 - accuracy: 0.9067 - val_loss: 0.2844 - val_accuracy: 0.8818\n",
      "Epoch 19/40\n",
      "150/150 [==============================] - 79s 526ms/step - loss: 0.2074 - accuracy: 0.9090 - val_loss: 0.2857 - val_accuracy: 0.8806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f57ec895790>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajustando o modelo:\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping('val_loss', patience=4, restore_best_weights=True)\n",
    "model.fit(train_tfd, epochs=40, validation_data=val_tfd, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esperamos chegar, em umas 19 épocas, em algo perto de:\n",
    "\n",
    "    loss: 0.2074 - accuracy: 0.9090 - val_loss: 0.2857 - val_accuracy: 0.8806|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgOnPTYo4OfW"
   },
   "outputs": [],
   "source": [
    "# Salva o modelo treinado:\n",
    "#model.save_pretrained('bertimbau-hatespeech-trained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5Spy4eo9Rn_"
   },
   "source": [
    "## Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QiBvQrUvFOSx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trg4qwKdgD8U",
    "outputId": "a786d823-1357-489e-eddc-f02fb671f468"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bertimbau-hatespeech-trained were not used when initializing TFBertForSequenceClassification: ['dropout_75']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at bertimbau-hatespeech-trained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "saved_model = TFAutoModelForSequenceClassification.from_pretrained('bertimbau-hatespeech-trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_Z9LnEukgtjG",
    "outputId": "26b797b1-4612-4d1e-f63b-837d6880c791"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.881\n",
      "f1: 0.512\n",
      "prec: 0.646\n",
      "rec: 0.424\n"
     ]
    }
   ],
   "source": [
    "# Predictions for validation set:\n",
    "val_pred  = predict_class(saved_model, val_tfd)\n",
    "\n",
    "# Metrics:\n",
    "y_true, y_pred = val_df['label'], val_pred\n",
    "for name, scorer in {'acc': accuracy_score, 'f1': f1_score, 'prec': precision_score, 'rec': recall_score}.items():\n",
    "    s = scorer(y_true, y_pred)\n",
    "    print('{}: {:.3f}'.format(name, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwspzRwmFbVX",
    "outputId": "e33454c9-a0d7-46dd-e313-9b42a9165f95"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.901\n",
      "f1: 0.596\n",
      "prec: 0.689\n",
      "rec: 0.525\n"
     ]
    }
   ],
   "source": [
    "# Predictions for test set:\n",
    "test_pred = predict_class(model, test_tfd)\n",
    "\n",
    "# Metrics:\n",
    "y_true, y_pred = test_df['label'], test_pred\n",
    "for name, scorer in {'acc': accuracy_score, 'f1': f1_score, 'prec': precision_score, 'rec': recall_score}.items():\n",
    "    s = scorer(y_true, y_pred)\n",
    "    print('{}: {:.3f}'.format(name, s))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "modelo_bert_com_tensorflow.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0a3754eebd83498eaf19c4dbd836d6fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "11b3d04dc0f3446d8b7d007aaa923865": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_204739ce239b47cb86643f556939e81f",
      "placeholder": "​",
      "style": "IPY_MODEL_0a3754eebd83498eaf19c4dbd836d6fb",
      "value": " 1/1 [00:00&lt;00:00,  4.78ba/s]"
     }
    },
    "204739ce239b47cb86643f556939e81f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28ac59e6f3ad4442837e470d63a5913a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28f9d87645184fca8b8305167b4c2539": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f51f1a3231d4b4eb20b407b5664f78f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8b6ca3c2ac2c4f4c88c9528f52f075a3",
      "placeholder": "​",
      "style": "IPY_MODEL_707bd0c951ff46d88df758e3dfc7faec",
      "value": "100%"
     }
    },
    "3fba13def095450f9c9b2dd0c65f5fd0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4e62dbd2382d4d15aa49cff8de61be53": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "585fa61fe96b4c11bf0163b3ea646cd2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6250c40f8e114db9a226db01ce2a1d84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bae291373ce24ce1a0594334ae2ce414",
       "IPY_MODEL_734e8736e4b948c790536a524aaf7f01",
       "IPY_MODEL_9d91c870729c4cfba700acda20eeef0d"
      ],
      "layout": "IPY_MODEL_4e62dbd2382d4d15aa49cff8de61be53"
     }
    },
    "707bd0c951ff46d88df758e3dfc7faec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "734e8736e4b948c790536a524aaf7f01": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_585fa61fe96b4c11bf0163b3ea646cd2",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_976513a8ec674d3fb16cb04d8166533a",
      "value": 5
     }
    },
    "7f0a57db827b4b47a492759ac218377d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b6ca3c2ac2c4f4c88c9528f52f075a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "976513a8ec674d3fb16cb04d8166533a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d91c870729c4cfba700acda20eeef0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfe43e79dae34624bad82ab5cab0e4bf",
      "placeholder": "​",
      "style": "IPY_MODEL_28f9d87645184fca8b8305167b4c2539",
      "value": " 5/5 [00:00&lt;00:00,  6.03ba/s]"
     }
    },
    "b2ae2782be834feeaffde4e50fc0e3f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "bae291373ce24ce1a0594334ae2ce414": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cd5d6dfa298b4756a6d4e714cdefedd5",
      "placeholder": "​",
      "style": "IPY_MODEL_b2ae2782be834feeaffde4e50fc0e3f2",
      "value": "100%"
     }
    },
    "bfe43e79dae34624bad82ab5cab0e4bf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cd5d6dfa298b4756a6d4e714cdefedd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d390ffb74d5044f8b0c847c30191957c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_28ac59e6f3ad4442837e470d63a5913a",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3fba13def095450f9c9b2dd0c65f5fd0",
      "value": 1
     }
    },
    "dd64a6ae604847808188a63f60b9dd6e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3f51f1a3231d4b4eb20b407b5664f78f",
       "IPY_MODEL_d390ffb74d5044f8b0c847c30191957c",
       "IPY_MODEL_11b3d04dc0f3446d8b7d007aaa923865"
      ],
      "layout": "IPY_MODEL_7f0a57db827b4b47a492759ac218377d"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
