{'memory': None, 'steps': [('proc', ColumnTransformer(transformers=[('txt',
                                 CountVectorizer(strip_accents='unicode'),
                                 'text')])), ('fit', ComplementNB(alpha=0.2))], 'verbose': False, 'proc': ColumnTransformer(transformers=[('txt',
                                 CountVectorizer(strip_accents='unicode'),
                                 'text')]), 'fit': ComplementNB(alpha=0.2), 'proc__n_jobs': None, 'proc__remainder': 'drop', 'proc__sparse_threshold': 0.3, 'proc__transformer_weights': None, 'proc__transformers': [('txt', CountVectorizer(strip_accents='unicode'), 'text')], 'proc__verbose': False, 'proc__verbose_feature_names_out': True, 'proc__txt': CountVectorizer(strip_accents='unicode'), 'proc__txt__analyzer': 'word', 'proc__txt__binary': False, 'proc__txt__decode_error': 'strict', 'proc__txt__dtype': <class 'numpy.int64'>, 'proc__txt__encoding': 'utf-8', 'proc__txt__input': 'content', 'proc__txt__lowercase': True, 'proc__txt__max_df': 1.0, 'proc__txt__max_features': None, 'proc__txt__min_df': 1, 'proc__txt__ngram_range': (1, 1), 'proc__txt__preprocessor': None, 'proc__txt__stop_words': None, 'proc__txt__strip_accents': 'unicode', 'proc__txt__token_pattern': '(?u)\\b\\w\\w+\\b', 'proc__txt__tokenizer': None, 'proc__txt__vocabulary': None, 'fit__alpha': 0.2, 'fit__class_prior': None, 'fit__fit_prior': True, 'fit__norm': False}